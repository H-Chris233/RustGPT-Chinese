use std::cmp::Ordering;
use std::collections::BinaryHeap;
use std::time::Instant;

use ndarray::{Array1, Array2, Array3, ArrayView1, Axis};
use rand::{rng, Rng};

use crate::{
    output_projection::OutputProjection,
    transformer::TransformerBlock,
    utils::{log_softmax, softmax},
    Embeddings, PerformanceMonitor, Vocab, EMBEDDING_DIM, HIDDEN_DIM, MAX_SEQ_LEN, SOFTMAX_EPSILON,
};

#[derive(Clone)]
struct ProbEntry {
    prob: f32,
    idx: usize,
}

impl PartialEq for ProbEntry {
    fn eq(&self, other: &Self) -> bool {
        self.idx == other.idx && self.prob.to_bits() == other.prob.to_bits()
    }
}

impl Eq for ProbEntry {}

impl PartialOrd for ProbEntry {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for ProbEntry {
    fn cmp(&self, other: &Self) -> Ordering {
        other
            .prob
            .total_cmp(&self.prob)
            .then_with(|| self.idx.cmp(&other.idx))
    }
}

#[derive(Clone)]
struct SessionSnapshot {
    processed_tokens: usize,
    kv_caches: Vec<Option<(Array2<f32>, Array2<f32>)>>,
}

pub struct InferenceSession<'a> {
    llm: &'a mut LLM,
    processed_tokens: usize,
    max_context_length: usize,
    temperature: f32,
    top_p: f32,
    top_k: usize,
    previous_training_mode: bool,
    kv_cache_was_enabled: bool,
}

impl<'a> InferenceSession<'a> {
    pub fn new(llm: &'a mut LLM, temperature: f32, top_p: f32, top_k: usize) -> Self {
        let previous_training_mode = llm.training;
        let kv_cache_was_enabled = llm.is_kv_cache_enabled();
        let max_context_length = llm.max_context_length;

        llm.set_training_mode(false);
        llm.enable_kv_cache();
        llm.clear_kv_cache();

        Self {
            llm,
            processed_tokens: 0,
            max_context_length,
            temperature,
            top_p,
            top_k,
            previous_training_mode,
            kv_cache_was_enabled,
        }
    }

    pub fn prime_tokens(&mut self, tokens: &[usize]) -> Option<Array2<f32>> {
        if tokens.is_empty() {
            return None;
        }

        self.llm.clear_kv_cache();
        self.processed_tokens = 0;

        let mut last_logits = None;
        for &token in tokens {
            last_logits = Some(self.advance_with_token(token));
        }

        last_logits
    }

    pub fn advance_with_token(&mut self, token_id: usize) -> Array2<f32> {
        if self.processed_tokens >= self.max_context_length {
            log::warn!(
                "ä¸Šä¸‹æ–‡é•¿åº¦è¶…è¿‡é˜ˆå€¼({}), è‡ªåŠ¨é‡ç½® KV ç¼“å­˜",
                self.max_context_length
            );
            self.llm.clear_kv_cache();
            self.processed_tokens = 0;
        }

        let logits = self.llm.inference_step(token_id, self.processed_tokens);
        self.processed_tokens += 1;
        logits
    }

    pub fn sample_next_token(&mut self, logits: &Array2<f32>) -> usize {
        let probs = softmax(logits);
        let adjusted = LLM::apply_temperature(&probs, self.temperature);

        let candidates = if self.top_k > 0 {
            self.llm.top_k_sampling(&adjusted, self.top_k)
        } else {
            self.llm.top_p_sampling(&adjusted, self.top_p)
        };

        candidates.into_iter().next().unwrap_or(0)
    }

    pub fn snapshot(&mut self) -> SessionSnapshot {
        SessionSnapshot {
            processed_tokens: self.processed_tokens,
            kv_caches: self.llm.capture_kv_cache(),
        }
    }

    pub fn restore(&mut self, snapshot: &SessionSnapshot) {
        self.processed_tokens = snapshot.processed_tokens;
        self.llm.restore_kv_cache(&snapshot.kv_caches);
    }

    pub fn processed_tokens(&self) -> usize {
        self.processed_tokens
    }
}

impl<'a> Drop for InferenceSession<'a> {
    fn drop(&mut self) {
        if self.previous_training_mode {
            self.llm.set_training_mode(true);
        } else {
            self.llm.set_training_mode(false);
        }

        if !self.kv_cache_was_enabled {
            self.llm.disable_kv_cache();
        }

        self.llm.clear_kv_cache();
    }
}

/// Layer trait - æ”¯æŒå•æ ·æœ¬å’Œæ‰¹é‡å¤„ç†
///
/// æ‰€æœ‰ç¥ç»ç½‘ç»œå±‚éœ€è¦å®ç°è¿™ä¸ªtraitï¼Œæ”¯æŒï¼š
/// - å•æ ·æœ¬å¤„ç†ï¼šforward/backward ä½¿ç”¨ Array2 (seq_len, hidden_dim)
/// - æ‰¹é‡å¤„ç†ï¼šforward_batch/backward_batch ä½¿ç”¨ Array3 (batch, seq, hidden_dim)
pub trait Layer {
    fn layer_type(&self) -> &str;

    /// å•æ ·æœ¬å‰å‘ä¼ æ’­ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32>;

    /// å•æ ·æœ¬åå‘ä¼ æ’­ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
    fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32>;

    /// ç”¨äºç±»å‹è½¬æ¢çš„è¾…åŠ©æ–¹æ³•
    fn as_any(&self) -> &dyn std::any::Any;
    fn as_any_mut(&mut self) -> &mut dyn std::any::Any;

    /// æ‰¹é‡å‰å‘ä¼ æ’­
    ///
    /// # å‚æ•°
    /// - `input`: (batch_size, seq_len, hidden_dim) æˆ– (batch_size, seq_len) å¯¹äºembeddings
    /// - `attention_mask`: å¯é€‰çš„æ³¨æ„åŠ›æ©ç  (batch_size, seq_len)ï¼Œ1.0è¡¨ç¤ºçœŸå®tokenï¼Œ0.0è¡¨ç¤ºPAD
    ///
    /// # è¿”å›å€¼
    /// (batch_size, seq_len, hidden_dim) çš„è¾“å‡ºå¼ é‡
    fn forward_batch(
        &mut self,
        input: &Array3<f32>,
        _attention_mask: Option<&Array2<f32>>,
    ) -> Array3<f32> {
        // é»˜è®¤å®ç°ï¼šå¯¹æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬åˆ†åˆ«è°ƒç”¨å•æ ·æœ¬ forward
        let batch_size = input.shape()[0];
        let seq_len = input.shape()[1];
        let hidden_dim = input.shape()[2];

        let mut output = Array3::zeros((batch_size, seq_len, hidden_dim));

        for b in 0..batch_size {
            let sample = input.slice(ndarray::s![b, .., ..]).to_owned();
            let sample_output = self.forward(&sample);
            output
                .slice_mut(ndarray::s![b, .., ..])
                .assign(&sample_output);
        }

        output
    }

    /// æ‰¹é‡åå‘ä¼ æ’­
    ///
    /// # å‚æ•°
    /// - `grads`: (batch_size, seq_len, hidden_dim) çš„æ¢¯åº¦
    /// - `lr`: å­¦ä¹ ç‡
    /// - `attention_mask`: å¯é€‰çš„æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºæ’é™¤PADä½ç½®çš„æ¢¯åº¦
    ///
    /// # è¿”å›å€¼
    /// (batch_size, seq_len, hidden_dim) çš„è¾“å…¥æ¢¯åº¦
    fn backward_batch(
        &mut self,
        grads: &Array3<f32>,
        lr: f32,
        attention_mask: Option<&Array2<f32>>,
    ) -> Array3<f32> {
        // é»˜è®¤å®ç°ï¼šå¯¹æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬åˆ†åˆ«è°ƒç”¨å•æ ·æœ¬ backward
        let batch_size = grads.shape()[0];
        let seq_len = grads.shape()[1];
        let hidden_dim = grads.shape()[2];

        let mut grad_input = Array3::zeros((batch_size, seq_len, hidden_dim));

        for b in 0..batch_size {
            let mut sample_grad = grads.slice(ndarray::s![b, .., ..]).to_owned();

            // å¦‚æœæœ‰æ³¨æ„åŠ›æ©ç ï¼Œå°†PADä½ç½®çš„æ¢¯åº¦æ¸…é›¶
            if let Some(mask) = attention_mask {
                for s in 0..seq_len {
                    if mask[[b, s]] < 0.5 {
                        // PADä½ç½®ï¼Œæ¢¯åº¦æ¸…é›¶
                        sample_grad.row_mut(s).fill(0.0);
                    }
                }
            }

            let sample_grad_input = self.backward(&sample_grad, lr);
            grad_input
                .slice_mut(ndarray::s![b, .., ..])
                .assign(&sample_grad_input);
        }

        grad_input
    }

    fn parameters(&self) -> usize;

    fn set_training_mode(&mut self, _training: bool) {}
}

#[allow(clippy::upper_case_acronyms)]
pub struct LLM {
    pub vocab: Vocab,
    pub network: Vec<Box<dyn Layer>>,
    pub context_window: Vec<usize>,
    pub max_context_length: usize,
    pub training: bool,
    pub parallel_training: bool,
    // æ€§èƒ½ä¼˜åŒ–ï¼šå¯é‡ç”¨çš„é‡‡æ ·ç¼“å†²åŒºï¼ˆpublicä»¥ä¾¿åºåˆ—åŒ–ï¼‰
    pub sampling_prob_buffer: Vec<f32>,
    pub sampling_idx_buffer: Vec<(f32, usize)>,
    pub beam_candidates_buffer: Vec<(Vec<usize>, f32)>,
}

/// æ—©åœæœºåˆ¶
///
/// ç›‘æ§è®­ç»ƒlossï¼Œå¦‚æœé•¿æ—¶é—´ä¸æ”¹å–„åˆ™è‡ªåŠ¨åœæ­¢è®­ç»ƒ
pub struct EarlyStopping {
    /// å®¹å¿å¤šå°‘ä¸ªepoch lossä¸æ”¹å–„
    patience: usize,

    /// å½“å‰æœ€ä½³loss
    best_loss: f32,

    /// å·²ç»å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„
    counter: usize,

    /// æœ€å°æ”¹å–„å¹…åº¦ï¼ˆå°äºè¿™ä¸ªå€¼ä¸ç®—æ”¹å–„ï¼‰
    min_delta: f32,

    /// æœ€ä½³æ¨¡å‹æ‰€åœ¨çš„epoch
    best_epoch: usize,
}

impl EarlyStopping {
    /// åˆ›å»ºæ—©åœç›‘æ§å™¨
    ///
    /// # å‚æ•°
    /// - `patience`: å®¹å¿epochæ•°ï¼ˆæ¨è30-50ï¼‰
    /// - `min_delta`: æœ€å°æ”¹å–„å¹…åº¦ï¼ˆæ¨è0.001ï¼‰
    pub fn new(patience: usize, min_delta: f32) -> Self {
        Self {
            patience,
            best_loss: f32::INFINITY,
            counter: 0,
            min_delta,
            best_epoch: 0,
        }
    }

    /// æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
    ///
    /// # è¿”å›å€¼
    /// - `true`: åº”è¯¥åœæ­¢è®­ç»ƒ
    /// - `false`: ç»§ç»­è®­ç»ƒ
    pub fn should_stop(&mut self, current_loss: f32, current_epoch: usize) -> bool {
        // å¦‚æœlossæœ‰æ˜æ˜¾æ”¹å–„
        if current_loss < self.best_loss - self.min_delta {
            self.best_loss = current_loss;
            self.best_epoch = current_epoch;
            self.counter = 0;
            false
        } else {
            // lossæ²¡æœ‰æ”¹å–„
            self.counter += 1;
            self.counter >= self.patience
        }
    }

    /// è·å–æœ€ä½³losså’Œå¯¹åº”çš„epoch
    pub fn best_state(&self) -> (f32, usize) {
        (self.best_loss, self.best_epoch)
    }
}

impl Default for LLM {
    fn default() -> Self {
        let transformer_block_1 = TransformerBlock::new(EMBEDDING_DIM, HIDDEN_DIM);
        let transformer_block_2 = TransformerBlock::new(EMBEDDING_DIM, HIDDEN_DIM);
        let transformer_block_3 = TransformerBlock::new(EMBEDDING_DIM, HIDDEN_DIM);
        let transformer_block_4 = TransformerBlock::new(EMBEDDING_DIM, HIDDEN_DIM);
        let output_projection = OutputProjection::new(EMBEDDING_DIM, Vocab::default_words().len());
        let vocab_size = Vocab::default_words().len();
        Self {
            vocab: Vocab::default(),
            network: vec![
                Box::new(Embeddings::default()),
                Box::new(transformer_block_1),
                Box::new(transformer_block_2),
                Box::new(transformer_block_3),
                Box::new(transformer_block_4),
                Box::new(output_projection),
            ],
            context_window: Vec::new(),
            max_context_length: MAX_SEQ_LEN,
            training: true,
            parallel_training: true,
            sampling_prob_buffer: Vec::with_capacity(vocab_size),
            sampling_idx_buffer: Vec::with_capacity(vocab_size),
            beam_candidates_buffer: Vec::with_capacity(50),
        }
    }
}

impl LLM {
    pub fn new(vocab: Vocab, network: Vec<Box<dyn Layer>>) -> Self {
        let vocab_size = vocab.words.len();
        Self {
            vocab,
            network,
            context_window: Vec::new(),
            max_context_length: MAX_SEQ_LEN,
            training: true,
            parallel_training: true,
            sampling_prob_buffer: Vec::with_capacity(vocab_size),
            sampling_idx_buffer: Vec::with_capacity(vocab_size),
            beam_candidates_buffer: Vec::with_capacity(50),
        }
    }
}

impl LLM {
    fn for_each_transformer_block_mut<F>(&mut self, mut f: F)
    where
        F: FnMut(&mut TransformerBlock),
    {
        for layer in &mut self.network {
            if let Some(block) = layer.as_any_mut().downcast_mut::<TransformerBlock>() {
                f(block);
            }
        }
    }

    fn for_each_transformer_block<F>(&self, mut f: F)
    where
        F: FnMut(&TransformerBlock),
    {
        for layer in &self.network {
            if let Some(block) = layer.as_any().downcast_ref::<TransformerBlock>() {
                f(block);
            }
        }
    }

    fn is_kv_cache_enabled(&self) -> bool {
        for layer in &self.network {
            if let Some(block) = layer.as_any().downcast_ref::<TransformerBlock>() {
                return block.attention.use_kv_cache;
            }
        }
        false
    }

    fn capture_kv_cache(&self) -> Vec<Option<(Array2<f32>, Array2<f32>)>> {
        let mut caches = Vec::new();
        self.for_each_transformer_block(|block| {
            caches.push(
                block
                    .attention
                    .kv_cache
                    .as_ref()
                    .map(|(k, v)| (k.clone(), v.clone())),
            );
        });
        caches
    }

    fn restore_kv_cache(&mut self, caches: &[Option<(Array2<f32>, Array2<f32>)>]) {
        let mut idx = 0usize;
        self.for_each_transformer_block_mut(|block| {
            if let Some(cache) = caches.get(idx) {
                block.attention.kv_cache = cache.as_ref().map(|(k, v)| (k.clone(), v.clone()));
            } else {
                block.attention.kv_cache = None;
            }
            idx += 1;
        });
    }

    fn inference_step(&mut self, token_id: usize, position: usize) -> Array2<f32> {
        let embedding_output = {
            let embeddings = self
                .network
                .first_mut()
                .expect("ç½‘ç»œè‡³å°‘åŒ…å«åµŒå…¥å±‚")
                .as_any_mut()
                .downcast_mut::<Embeddings>()
                .expect("é¦–å±‚å¿…é¡»æ˜¯ Embeddings");
            embeddings.embed_tokens_with_offset(&[token_id], position)
        };

        let mut hidden = embedding_output;

        for layer in self.network.iter_mut().skip(1) {
            if let Some(block) = layer.as_any_mut().downcast_mut::<TransformerBlock>() {
                hidden = block.forward_inference(&hidden);
            } else {
                hidden = layer.forward(&hidden);
            }
        }

        hidden
    }

    fn select_top_k_from_row(&mut self, row: ArrayView1<'_, f32>, k: usize) -> Vec<(usize, f32)> {
        self.sampling_idx_buffer.clear();
        self.sampling_idx_buffer
            .extend(row.iter().enumerate().map(|(idx, &prob)| (prob, idx)));

        let top_k = k.min(self.sampling_idx_buffer.len());
        if top_k == 0 {
            return Vec::new();
        }

        let nth = top_k - 1;
        self.sampling_idx_buffer
            .select_nth_unstable_by(nth, |a, b| b.0.partial_cmp(&a.0).unwrap_or(Ordering::Equal));

        let top_slice = &mut self.sampling_idx_buffer[..top_k];
        top_slice.sort_unstable_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(Ordering::Equal));
        top_slice.iter().map(|&(prob, idx)| (idx, prob)).collect()
    }

    fn supports_incremental(&self) -> bool {
        self.network
            .first()
            .and_then(|layer| layer.as_any().downcast_ref::<Embeddings>())
            .is_some()
    }

    pub fn network_description(&self) -> String {
        self.network
            .iter()
            .map(|layer| layer.layer_type())
            .collect::<Vec<&str>>()
            .join(", ")
    }

    pub fn total_parameters(&self) -> usize {
        self.network
            .iter()
            .map(|layer| layer.parameters())
            .sum::<usize>()
    }

    pub fn set_training_mode(&mut self, training: bool) {
        self.training = training;
        for layer in &mut self.network {
            layer.set_training_mode(training);
        }
    }

    pub fn set_parallel_training(&mut self, enabled: bool) {
        self.parallel_training = enabled;
    }

    pub fn parallel_training_enabled(&self) -> bool {
        self.parallel_training
    }

    #[allow(dead_code)]
    pub fn predict(&mut self, text: &str) -> String {
        self.predict_with_sampling(text, 1.0, 0.9, 5)
    }

    #[allow(dead_code)]
    pub fn predict_with_sampling(
        &mut self,
        text: &str,
        temperature: f32,
        top_p: f32,
        top_k: usize,
    ) -> String {
        let prompt_tokens = self.tokenize(text);
        let max_new_tokens = MAX_SEQ_LEN.saturating_sub(prompt_tokens.len());
        let generated_tokens = self.generate_tokens_incremental(
            &prompt_tokens,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        );

        self.tokens_to_text(&generated_tokens)
    }

    pub fn predict_with_context(
        &mut self,
        text: &str,
        temperature: f32,
        top_p: f32,
        top_k: usize,
    ) -> String {
        let new_tokens = self.tokenize(text);

        let mut combined_tokens = self.context_window.clone();
        combined_tokens.extend_from_slice(&new_tokens);

        if combined_tokens.len() > self.max_context_length {
            let start_idx = combined_tokens.len() - self.max_context_length;
            combined_tokens = combined_tokens[start_idx..].to_vec();
        }

        let available = self
            .max_context_length
            .saturating_sub(combined_tokens.len());
        let max_new_tokens = available.min(20);
        let generated_tokens = self.generate_tokens_incremental(
            &combined_tokens,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        );

        self.add_to_context(&new_tokens);
        self.add_to_context(&generated_tokens);

        self.tokens_to_text(&generated_tokens)
    }

    pub fn predict_with_beam_search(
        &mut self,
        text: &str,
        beam_width: usize,
        max_length: usize,
    ) -> String {
        self.beam_search(text, beam_width, max_length)
    }

    #[allow(dead_code)]
    fn forward_with_sampling(
        &mut self,
        text: &str,
        temperature: f32,
        top_p: f32,
        top_k: usize,
    ) -> String {
        let prompt_tokens = self.tokenize(text);
        let max_new_tokens = MAX_SEQ_LEN.saturating_sub(prompt_tokens.len());
        let generated_tokens = self.generate_tokens_incremental(
            &prompt_tokens,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        );

        self.tokens_to_text(&generated_tokens)
    }

    #[allow(dead_code)]
    fn forward(&mut self, text: &str) -> Vec<usize> {
        let mut tokenized = self.tokenize(text);
        let mut output_tokens: Vec<usize> = Vec::new();

        if tokenized.is_empty() {
            return output_tokens;
        }

        let input_len = tokenized.len();

        if input_len >= MAX_SEQ_LEN {
            return output_tokens;
        }

        for _ in 0..(MAX_SEQ_LEN - input_len) {
            if output_tokens.len() >= MAX_SEQ_LEN - 1 {
                break;
            }

            let token_input = Array2::from_shape_vec(
                (1, tokenized.len()),
                tokenized.iter().map(|&x| x as f32).collect(),
            )
            .unwrap();
            let mut input = token_input;

            for layer in &mut self.network {
                input = layer.forward(&input);
            }

            let logits = input;

            if logits.shape()[0] == 0 {
                break;
            }

            let last_logit = logits
                .row(logits.shape()[0] - 1)
                .to_owned()
                .insert_axis(Axis(0));

            let probs = softmax(&last_logit);

            let tokens = Self::greedy_decode(&probs);

            let next_token = tokens[tokens.len() - 1];

            output_tokens.push(next_token);
            tokenized.push(next_token);

            if next_token == self.vocab.eos_token_id() {
                break;
            }
        }

        output_tokens
    }

    pub fn train(&mut self, data: Vec<&str>, epochs: usize, initial_lr: f32) {
        self.set_training_mode(true);

        let tokenized_data = data
            .iter()
            .map(|input| self.tokenize(input))
            .collect::<Vec<Vec<usize>>>();

        for epoch in 0..epochs {
            let decay_rate: f32 = 0.95;
            let decay_steps = 10.0;
            let current_lr = initial_lr * decay_rate.powf(epoch as f32 / decay_steps);

            let mut total_loss = 0.0;
            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                // 1. Slice input and targets
                let input_ids = &training_row[..training_row.len() - 1]; // Exclude the last token
                let target_ids = &training_row[1..]; // This is a vector. Each element is the index in the vocab. 

                // Forward pass
                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let log_probs = log_softmax(&logits);
                total_loss += Self::cross_entropy_from_log_probs(&log_probs, target_ids);

                // Backward pass: grad = softmax(logits) - one_hot
                let probs = log_probs.mapv(|x| x.exp());
                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);

                Self::clip_gradients(&mut grads_output, 1.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }

                let tokens = Self::greedy_decode(&probs);
                let next_token = tokens[tokens.len() - 1];

                if next_token == self.vocab.encode("</s>").unwrap() {
                    continue;
                }
            }

            println!(
                "Epoch {}: Loss = {:.4}, LR = {:.6}",
                epoch,
                total_loss / tokenized_data.len() as f32,
                current_lr
            );
        }

        self.set_training_mode(false);
    }

    /// ä½¿ç”¨é¢„tokenizeçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    ///
    /// è¿™ä¸ªæ–¹æ³•æ¥å—å·²ç»tokenizeçš„æ•°æ®ï¼Œé¿å…é‡å¤tokenization
    pub fn train_with_cached_tokens(
        &mut self,
        tokenized_data: Vec<Vec<usize>>,
        epochs: usize,
        initial_lr: f32,
    ) {
        self.set_training_mode(true);

        for epoch in 0..epochs {
            let decay_rate: f32 = 0.95;
            let decay_steps = 10.0;
            let current_lr = initial_lr * decay_rate.powf(epoch as f32 / decay_steps);

            let mut total_loss = 0.0;
            let mut sample_count = 0;

            // ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„tokenizedæ•°æ®ï¼Œæ— éœ€é‡å¤tokenize
            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                // å‰å‘ä¼ æ’­
                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                // ä½¿ç”¨ log_softmax + NLL æå‡æ•°å€¼ç¨³å®šæ€§
                let log_probs = log_softmax(&logits);
                total_loss += Self::cross_entropy_from_log_probs(&log_probs, target_ids);

                // åå‘ä¼ æ’­ï¼šgrad = softmax(logits) - one_hot
                let probs = log_probs.mapv(|x| x.exp());
                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);

                // æ›´å¼ºçš„æ¢¯åº¦è£å‰ªæå‡ç¨³å®šæ€§
                Self::clip_gradients(&mut grads_output, 1.0);

                for layer in self.network.iter_mut().rev() {
                    grads_output = layer.backward(&grads_output, current_lr);
                }

                sample_count += 1;
            }

            println!(
                "Epoch {}: Loss = {:.4}, LR = {:.6}",
                epoch,
                if sample_count > 0 {
                    total_loss / sample_count as f32
                } else {
                    0.0
                },
                current_lr
            );
        }

        self.set_training_mode(false);
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ğŸš€ é˜¶æ®µ1è®­ç»ƒä¼˜åŒ– - æ€§èƒ½ä¼˜åŒ–æ–¹æ³•
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    /// ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¸¦é‡å¯ï¼‰
    ///
    /// # å‚æ•°
    /// - `initial_lr`: åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚ 0.001ï¼‰
    /// - `epoch`: å½“å‰epoch
    /// - `total_epochs`: æ€»epochæ•°
    /// - `num_restarts`: é‡å¯æ¬¡æ•°ï¼ˆå¦‚2è¡¨ç¤ºè®­ç»ƒåˆ†ä¸º3ä¸ªå‘¨æœŸï¼‰
    ///
    /// # ç¤ºä¾‹
    /// ```rust
    /// use llm::LLM;
    /// // 500 epochs, 2æ¬¡é‡å¯ï¼Œæ¯ä¸ªå‘¨æœŸçº¦166 epochs
    /// let lr = LLM::cosine_annealing_lr(0.001, 100, 500, 2);
    /// ```
    pub fn cosine_annealing_lr(
        initial_lr: f32,
        epoch: usize,
        total_epochs: usize,
        num_restarts: usize,
    ) -> f32 {
        // è®¡ç®—æ¯ä¸ªå‘¨æœŸçš„é•¿åº¦
        let cycle_length = total_epochs / (num_restarts + 1);

        // å½“å‰åœ¨å‘¨æœŸå†…çš„ä½ç½®
        let cycle_epoch = epoch % cycle_length;

        // å‘¨æœŸå†…çš„è¿›åº¦ [0, 1]
        let progress = cycle_epoch as f32 / cycle_length as f32;

        // æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹å€¼çš„1%
        let min_lr = initial_lr * 0.01;

        // ä½™å¼¦é€€ç«å…¬å¼
        min_lr + 0.5 * (initial_lr - min_lr) * (1.0 + (std::f32::consts::PI * progress).cos())
    }

    /// è®¡ç®—æ¢¯åº¦L2èŒƒæ•°
    pub fn compute_grad_norm(grads: &Array2<f32>) -> f32 {
        grads.iter().map(|&x| x * x).sum::<f32>().sqrt()
    }

    /// å®Œæ•´ä¼˜åŒ–çš„è®­ç»ƒæ–¹æ³•ï¼ˆé›†æˆå¹¶è¡Œé¢„å¤„ç†ä¸ç›‘æ§ï¼‰
    ///
    /// # ç‰¹æ€§
    /// - âœ… æ•°æ®é¢„å¤„ç†ç¼“å­˜ï¼ˆé¿å…é‡å¤ tokenizationï¼‰
    /// - âœ… Rayon å¹¶è¡Œ tokenizationï¼ˆå¯æ ¹æ®æ•°æ®é‡è‡ªåŠ¨å›é€€ï¼‰
    /// - âœ… ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
    /// - âœ… æ—©åœæœºåˆ¶
    /// - âœ… å¢å¼ºè®­ç»ƒç›‘æ§ï¼ˆå›°æƒ‘åº¦ã€æ¢¯åº¦èŒƒæ•°ã€è®­ç»ƒé€Ÿåº¦ï¼‰
    /// - âœ… Rayon scope æ¢¯åº¦å½’çº¦ï¼ˆå¯æ ¹æ®æ•°æ®é‡è‡ªåŠ¨å›é€€ï¼‰
    ///
    /// # å‚æ•°
    /// - `data`: è®­ç»ƒæ•°æ®
    /// - `max_epochs`: æœ€å¤§ epoch æ•°
    /// - `initial_lr`: åˆå§‹å­¦ä¹ ç‡
    /// - `patience`: æ—©åœå®¹å¿ epoch æ•°
    /// - `accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨è 4-8ï¼‰
    ///
    /// # è¿”å›å€¼
    /// å®é™…è®­ç»ƒçš„ epoch æ•°
    pub fn train_monitored(
        &mut self,
        data: Vec<&str>,
        max_epochs: usize,
        initial_lr: f32,
        patience: usize,
        accumulation_steps: usize,
    ) -> usize {
        self.set_training_mode(true);

        const MIN_PARALLEL_TOKENIZE: usize = 16;
        const MIN_PARALLEL_GRAD: usize = 8;

        let mut perf_monitor = PerformanceMonitor::new();
        let effective_accum_steps = accumulation_steps.max(1);

        println!("ğŸ“ æ­£åœ¨é¢„å¤„ç†è®­ç»ƒæ•°æ®...");
        let preprocess_start = std::time::Instant::now();

        let should_parallel_preprocess =
            self.parallel_training && data.len() >= MIN_PARALLEL_TOKENIZE;

        let preprocess_label = if should_parallel_preprocess {
            "tokenization_parallel"
        } else {
            "tokenization_single_thread"
        };

        perf_monitor.start(preprocess_label);
        let tokenized_data: Vec<Vec<usize>> = data
            .iter()
            .map(|input| Self::tokenize_with_vocab(&self.vocab, input))
            .collect();
        perf_monitor.stop(preprocess_label);

        println!(
            "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå…± {} ä¸ªåºåˆ—ï¼ˆè€—æ—¶ {:.2}sï¼‰",
            tokenized_data.len(),
            preprocess_start.elapsed().as_secs_f32()
        );

        if self.parallel_training && !should_parallel_preprocess {
            println!("âš ï¸  æ ·æœ¬æ•°è¾ƒå°‘ï¼Œtokenization è‡ªåŠ¨å›é€€ä¸ºå•çº¿ç¨‹æ¨¡å¼");
        }

        let use_parallel_gradients = self.parallel_training
            && effective_accum_steps > 1
            && tokenized_data.len() >= MIN_PARALLEL_GRAD;

        println!(
            "ğŸ§µ æ¢¯åº¦å½’çº¦æ¨¡å¼: {} (accumulation_steps={})",
            if use_parallel_gradients {
                "rayon å¹¶è¡Œ"
            } else if self.parallel_training {
                "å•çº¿ç¨‹ï¼ˆè‡ªåŠ¨å›é€€ï¼‰"
            } else {
                "å•çº¿ç¨‹ï¼ˆæ‰‹åŠ¨é…ç½®ï¼‰"
            },
            effective_accum_steps
        );

        let mut early_stopping = EarlyStopping::new(patience, 0.01);
        let training_start_time = std::time::Instant::now();

        for epoch in 0..max_epochs {
            let epoch_start = std::time::Instant::now();

            // ğŸ”¥ ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆç¦ç”¨é‡å¯ä»¥æå‡ç¨³å®šæ€§ï¼‰
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, max_epochs, 0);

            let mut total_loss = 0.0;
            let mut total_grad_norm = 0.0;
            let mut sample_count = 0usize;

            let mut gradient_bucket: Vec<Array2<f32>> = Vec::with_capacity(effective_accum_steps);
            let mut bucket_expected_len: Option<usize> = None;

            for training_row in &tokenized_data {
                if training_row.len() < 2 {
                    continue;
                }

                let seq_len = training_row.len() - 1;

                if let Some(expected) = bucket_expected_len {
                    if expected != seq_len && !gradient_bucket.is_empty() {
                        self.apply_accumulated_gradients(
                            &mut gradient_bucket,
                            current_lr,
                            use_parallel_gradients,
                            &mut perf_monitor,
                        );
                        bucket_expected_len = None;
                    }
                }

                // å‰å‘ä¼ æ’­
                let input_ids = &training_row[..training_row.len() - 1];
                let target_ids = &training_row[1..];

                let mut input: Array2<f32> = Array2::zeros((1, input_ids.len()));
                input
                    .row_mut(0)
                    .assign(&input_ids.iter().map(|&x| x as f32).collect::<Array1<f32>>());

                for layer in &mut self.network {
                    input = layer.forward(&input);
                }

                let logits = input;
                let log_probs = log_softmax(&logits);
                total_loss += Self::cross_entropy_from_log_probs(&log_probs, target_ids);

                // è®¡ç®—è¾“å‡ºæ¢¯åº¦
                let probs = log_probs.mapv(|x| x.exp());
                let mut grads_output = Self::compute_gradients_step(&probs, target_ids);

                total_grad_norm += Self::compute_grad_norm(&grads_output);

                Self::clip_gradients(&mut grads_output, 1.0);

                bucket_expected_len.get_or_insert(seq_len);
                gradient_bucket.push(grads_output);

                if gradient_bucket.len() >= effective_accum_steps {
                    self.apply_accumulated_gradients(
                        &mut gradient_bucket,
                        current_lr,
                        use_parallel_gradients,
                        &mut perf_monitor,
                    );
                    bucket_expected_len = None;
                }

                sample_count += 1;
            }

            if !gradient_bucket.is_empty() {
                self.apply_accumulated_gradients(
                    &mut gradient_bucket,
                    current_lr,
                    use_parallel_gradients,
                    &mut perf_monitor,
                );
            }

            let epoch_time = epoch_start.elapsed().as_secs_f32();
            let avg_loss = if sample_count > 0 {
                total_loss / sample_count as f32
            } else {
                0.0
            };
            let avg_grad_norm = if sample_count > 0 {
                total_grad_norm / sample_count as f32
            } else {
                0.0
            };
            let perplexity = avg_loss.exp();
            let samples_per_sec = if epoch_time > 0.0 {
                sample_count as f32 / epoch_time
            } else {
                0.0
            };

            if epoch % 10 == 0 || epoch == max_epochs - 1 {
                let progress = (epoch + 1) as f32 / max_epochs as f32 * 100.0;
                let elapsed = training_start_time.elapsed().as_secs();
                let eta = if epoch + 1 > 0 {
                    (elapsed as f32 / (epoch + 1) as f32 * (max_epochs - epoch - 1) as f32) as u64
                } else {
                    0
                };

                println!(
                    "[{:3}/{}] {:6.1}% | Loss: {:.4} | PPL: {:6.2} | LR: {:.6} | Grad: {:6.4} | Speed: {:5.1} samples/s | ETA: {}s",
                    epoch + 1,
                    max_epochs,
                    progress,
                    avg_loss,
                    perplexity,
                    current_lr,
                    avg_grad_norm,
                    samples_per_sec,
                    eta
                );
            }

            if early_stopping.should_stop(avg_loss, epoch) {
                let (best_loss, best_epoch) = early_stopping.best_state();
                println!("\nğŸ›‘ æ—©åœè§¦å‘:");
                println!("   â€¢ æœ€ä½³epoch: {}", best_epoch + 1);
                println!("   â€¢ æœ€ä½³loss: {:.4}", best_loss);
                println!("   â€¢ åœæ­¢epoch: {}", epoch + 1);
                println!("   â€¢ èŠ‚çœæ—¶é—´: {} epochs", max_epochs - epoch);

                self.set_training_mode(false);
                perf_monitor.print_report();
                return epoch + 1;
            }
        }

        self.set_training_mode(false);
        perf_monitor.print_report();
        max_epochs
    }

    /// æ‰¹é‡è®­ç»ƒæ–¹æ³•ï¼ˆæ”¯æŒåŠ¨æ€æ©ç ï¼‰
    ///
    /// # ç‰¹æ€§
    /// - âœ… æ‰¹é‡å¤„ç†ï¼šæ˜¾è‘—æå‡è®­ç»ƒé€Ÿåº¦
    /// - âœ… åŠ¨æ€å¡«å……ï¼šæ¯ä¸ªæ‰¹æ¬¡å¡«å……åˆ°è¯¥æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦
    /// - âœ… æ³¨æ„åŠ›æ©ç ï¼šç¡®ä¿ PAD ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
    /// - âœ… æ•°æ®åˆ†æ¡¶ï¼šå‡å°‘å¡«å……å¼€é”€
    /// - âœ… æ‰€æœ‰ç›‘æ§å’Œä¼˜åŒ–ç‰¹æ€§ï¼ˆä½™å¼¦é€€ç«ã€æ—©åœç­‰ï¼‰
    ///
    /// # å‚æ•°
    /// - `data`: è®­ç»ƒæ•°æ®
    /// - `max_epochs`: æœ€å¤§ epoch æ•°
    /// - `initial_lr`: åˆå§‹å­¦ä¹ ç‡
    /// - `patience`: æ—©åœå®¹å¿ epoch æ•°
    /// - `batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆæ¨è 2-8ï¼‰
    ///
    /// # è¿”å›å€¼
    /// å®é™…è®­ç»ƒçš„ epoch æ•°
    pub fn train_monitored_batch(
        &mut self,
        data: Vec<&str>,
        max_epochs: usize,
        initial_lr: f32,
        patience: usize,
        batch_size: usize,
    ) -> usize {
        use crate::batch_loader::{BatchLoader, create_training_batches};

        self.set_training_mode(true);

        let perf_monitor = PerformanceMonitor::new();

        println!("ğŸ“ æ­£åœ¨é¢„å¤„ç†è®­ç»ƒæ•°æ®...");
        let preprocess_start = std::time::Instant::now();

        // Tokenize æ‰€æœ‰æ•°æ®
        let tokenized_data: Vec<Vec<usize>> = data
            .iter()
            .map(|input| Self::tokenize_with_vocab(&self.vocab, input))
            .collect();

        println!(
            "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå…± {} ä¸ªåºåˆ—ï¼ˆè€—æ—¶ {:.2}sï¼‰",
            tokenized_data.len(),
            preprocess_start.elapsed().as_secs_f32()
        );

        // åˆ›å»ºæ‰¹é‡åŠ è½½å™¨
        let batch_loader = BatchLoader::new(batch_size, true, 16);

        let mut early_stopping = EarlyStopping::new(patience, 0.01);
        let training_start_time = std::time::Instant::now();

        for epoch in 0..max_epochs {
            let epoch_start = std::time::Instant::now();

            // ä½™å¼¦é€€ç«å­¦ä¹ ç‡
            let current_lr = Self::cosine_annealing_lr(initial_lr, epoch, max_epochs, 0);

            let mut total_loss = 0.0;
            let mut total_grad_norm = 0.0;
            let mut sample_count = 0usize;

            // åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡
            let training_batches = create_training_batches(&batch_loader, &tokenized_data);

            for (input_batch, targets) in training_batches {
                // è·³è¿‡ç©ºæ‰¹æ¬¡
                if input_batch.batch_size == 0 {
                    continue;
                }

                // å‰å‘ä¼ æ’­ï¼ˆæ‰¹é‡ï¼‰- ä½¿ç”¨å¾ªç¯å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬å¤„ç†
                let mut batch_outputs = Vec::with_capacity(input_batch.batch_size);

                for b in 0..input_batch.batch_size {
                    let sample_tokens = input_batch.tokens.row(b);
                    let sample_ids: Vec<usize> = sample_tokens.to_vec();

                    // å•æ ·æœ¬å‰å‘ä¼ æ’­
                    let mut input: Array2<f32> = Array2::zeros((1, sample_ids.len()));
                    input.row_mut(0).assign(
                        &sample_ids
                            .iter()
                            .map(|&x| x as f32)
                            .collect::<Array1<f32>>(),
                    );

                    for layer in &mut self.network {
                        input = layer.forward(&input);
                    }

                    batch_outputs.push(input);
                }

                // è®¡ç®—æŸå¤±å’Œåå‘ä¼ æ’­ï¼ˆå¯¹æ¯ä¸ªæ ·æœ¬ï¼‰
                let mut batch_loss = 0.0;

                for (b, logits) in batch_outputs.iter().enumerate() {
                    if b >= targets.len() || targets[b].is_empty() {
                        continue;
                    }

                    let log_probs = log_softmax(logits);

                    // åªè®¡ç®—é PAD ä½ç½®çš„æŸå¤±
                    let target_ids = &targets[b];
                    let loss = Self::cross_entropy_from_log_probs(&log_probs, target_ids);
                    batch_loss += loss;

                    // è®¡ç®—æ¢¯åº¦
                    let probs = log_probs.mapv(|x| x.exp());
                    let mut sample_grad = Self::compute_gradients_step(&probs, target_ids);

                    // åº”ç”¨æ³¨æ„åŠ›æ©ç åˆ°æ¢¯åº¦ï¼ˆå°†PADä½ç½®æ¢¯åº¦æ¸…é›¶ï¼‰
                    for s in 0..sample_grad.nrows() {
                        if input_batch.attention_mask[[b, s]] < 0.5 {
                            sample_grad.row_mut(s).fill(0.0);
                        }
                    }

                    total_grad_norm += Self::compute_grad_norm(&sample_grad);
                    Self::clip_gradients(&mut sample_grad, 1.0);

                    // åå‘ä¼ æ’­
                    let mut grads = sample_grad;
                    for layer in self.network.iter_mut().rev() {
                        grads = layer.backward(&grads, current_lr);
                    }

                    sample_count += 1;
                }

                total_loss += batch_loss;
            }

            let epoch_time = epoch_start.elapsed().as_secs_f32();
            let avg_loss = if sample_count > 0 {
                total_loss / sample_count as f32
            } else {
                0.0
            };
            let avg_grad_norm = if sample_count > 0 {
                total_grad_norm / sample_count as f32
            } else {
                0.0
            };
            let perplexity = avg_loss.exp();
            let samples_per_sec = if epoch_time > 0.0 {
                sample_count as f32 / epoch_time
            } else {
                0.0
            };

            if epoch % 10 == 0 || epoch == max_epochs - 1 {
                let progress = (epoch + 1) as f32 / max_epochs as f32 * 100.0;
                let elapsed = training_start_time.elapsed().as_secs();
                let eta = if epoch + 1 > 0 {
                    (elapsed as f32 / (epoch + 1) as f32 * (max_epochs - epoch - 1) as f32) as u64
                } else {
                    0
                };

                println!(
                    "[{:3}/{}] {:6.1}% | Loss: {:.4} | PPL: {:6.2} | LR: {:.6} | Grad: {:6.4} | Speed: {:5.1} samples/s | ETA: {}s | Batch: {}",
                    epoch + 1,
                    max_epochs,
                    progress,
                    avg_loss,
                    perplexity,
                    current_lr,
                    avg_grad_norm,
                    samples_per_sec,
                    eta,
                    batch_size
                );
            }

            if early_stopping.should_stop(avg_loss, epoch) {
                let (best_loss, best_epoch) = early_stopping.best_state();
                println!("\nğŸ›‘ æ—©åœè§¦å‘:");
                println!("   â€¢ æœ€ä½³epoch: {}", best_epoch + 1);
                println!("   â€¢ æœ€ä½³loss: {:.4}", best_loss);
                println!("   â€¢ åœæ­¢epoch: {}", epoch + 1);
                println!("   â€¢ èŠ‚çœæ—¶é—´: {} epochs", max_epochs - epoch);

                self.set_training_mode(false);
                perf_monitor.print_report();
                return epoch + 1;
            }
        }

        self.set_training_mode(false);
        perf_monitor.print_report();
        max_epochs
    }

    /// è®¡ç®— 3D æ¢¯åº¦å¼ é‡çš„ L2 èŒƒæ•°
    fn compute_grad_norm_3d(grads: &Array3<f32>) -> f32 {
        grads.iter().map(|&x| x * x).sum::<f32>().sqrt()
    }

    /// 3D æ¢¯åº¦è£å‰ª
    fn clip_gradients_3d(grads: &mut Array3<f32>, max_norm: f32) {
        let norm = Self::compute_grad_norm_3d(grads);
        if norm > max_norm {
            let scale = max_norm / norm;
            grads.mapv_inplace(|x| x * scale);
        }
    }

    /// Add tokens to the context window, maintaining the maximum length
    pub fn add_to_context(&mut self, tokens: &[usize]) {
        // Add new tokens to the context window
        self.context_window.extend_from_slice(tokens);

        // If context exceeds maximum length, remove oldest tokens
        if self.context_window.len() > self.max_context_length {
            let excess = self.context_window.len() - self.max_context_length;
            self.context_window.drain(0..excess);
        }
    }

    /// Clear the context window
    pub fn clear_context(&mut self) {
        self.context_window.clear();
    }

    /// å¯ç”¨æ‰€æœ‰transformerå±‚çš„KVç¼“å­˜
    ///
    /// KVç¼“å­˜å¯ä»¥æ˜¾è‘—åŠ é€Ÿæ¨ç†é€Ÿåº¦ï¼ˆ10-100å€ï¼‰ï¼Œä½†ä¸èƒ½ç”¨äºè®­ç»ƒã€‚
    /// é€‚ç”¨åœºæ™¯ï¼šäº¤äº’å¼å¯¹è¯ç”Ÿæˆã€é€tokenç”Ÿæˆç­‰
    pub fn enable_kv_cache(&mut self) {
        self.for_each_transformer_block_mut(|block| block.attention.enable_kv_cache());
    }

    /// ç¦ç”¨æ‰€æœ‰transformerå±‚çš„KVç¼“å­˜
    pub fn disable_kv_cache(&mut self) {
        self.for_each_transformer_block_mut(|block| block.attention.disable_kv_cache());
    }

    /// æ¸…ç©ºæ‰€æœ‰transformerå±‚çš„KVç¼“å­˜ï¼ˆä¿æŒå¯ç”¨çŠ¶æ€ï¼‰
    pub fn clear_kv_cache(&mut self) {
        self.for_each_transformer_block_mut(|block| block.attention.clear_kv_cache());
    }

    /// ç»Ÿä¸€è®¾ç½®æ‰€æœ‰ TransformerBlock çš„ SelfAttention æ˜¯å¦å†»ç»“å‚æ•°æ›´æ–°
    /// ç”¨äºåœ¨ä¸ä¿®æ”¹ç½‘ç»œç»“æ„çš„å‰æä¸‹ï¼Œå¿«é€Ÿæ’æŸ¥è®­ç»ƒä¸ç¨³å®šé—®é¢˜
    pub fn set_attention_freeze_updates(&mut self, freeze: bool) {
        self.for_each_transformer_block_mut(|block| block.attention.freeze_updates = freeze);
    }

    /// Get current context as token IDs
    #[allow(dead_code)]
    pub fn get_context(&self) -> &[usize] {
        &self.context_window
    }

    /// Set a fixed context
    #[allow(dead_code)]
    pub fn set_context(&mut self, tokens: Vec<usize>) {
        self.context_window = tokens;
        // Ensure context doesn't exceed maximum length
        if self.context_window.len() > self.max_context_length {
            let excess = self.context_window.len() - self.max_context_length;
            self.context_window.drain(0..excess);
        }
    }

    pub fn tokenize_with_vocab(vocab: &Vocab, text: &str) -> Vec<usize> {
        let has_chinese = text
            .chars()
            .any(|c| (c as u32) >= 0x4E00 && (c as u32) <= 0x9FFF);

        if has_chinese {
            return vocab.encode_sequence(text);
        }

        let mut tokens = Vec::new();

        for word in text.split_whitespace() {
            if word == "</s>" {
                if let Some(token_id) = vocab.encode(word) {
                    tokens.push(token_id);
                }
                continue;
            }

            let mut current_word = String::new();

            for c in word.chars() {
                if c.is_ascii_punctuation() {
                    if !current_word.is_empty() {
                        if let Some(token_id) = vocab.encode(&current_word) {
                            tokens.push(token_id);
                        }
                        current_word.clear();
                    }

                    if let Some(token_id) = vocab.encode(&c.to_string()) {
                        tokens.push(token_id);
                    }
                } else {
                    current_word.push(c);
                }
            }

            if !current_word.is_empty()
                && let Some(token_id) = vocab.encode(&current_word)
            {
                tokens.push(token_id);
            }
        }

        tokens
    }

    pub fn tokenize(&self, text: &str) -> Vec<usize> {
        Self::tokenize_with_vocab(&self.vocab, text)
    }

    fn apply_temperature(probs: &Array2<f32>, temperature: f32) -> Array2<f32> {
        if temperature <= 0.0 {
            return probs.clone();
        }

        let power = 1.0 / temperature;
        let mut adjusted = probs.clone();

        for mut row in adjusted.rows_mut() {
            let mut sum = 0.0;
            for value in row.iter_mut() {
                *value = (*value).max(SOFTMAX_EPSILON).powf(power);
                sum += *value;
            }

            if sum > 0.0 {
                for value in row.iter_mut() {
                    *value /= sum;
                }
            }
        }

        adjusted
    }

    fn greedy_decode(probs: &Array2<f32>) -> Vec<usize> {
        probs
            .map_axis(Axis(1), |row| {
                row.iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap_or(Ordering::Equal))
                    .map(|(index, _)| index)
                    .unwrap_or(0)
            })
            .to_vec()
    }

    /// Top-k sampling: only consider the k most probable tokens
    /// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¤ç”¨å†…éƒ¨ç¼“å†²åŒºå‡å°‘åˆ†é…
    fn top_k_sampling(&mut self, probs: &Array2<f32>, k: usize) -> Vec<usize> {
        let mut result = Vec::with_capacity(probs.nrows());

        for row in probs.rows() {
            let top_entries = self.select_top_k_from_row(row, k);

            self.sampling_prob_buffer.clear();
            self.sampling_prob_buffer
                .resize(self.vocab.words.len(), 0.0);

            let mut sum = 0.0;
            for (idx, prob) in &top_entries {
                self.sampling_prob_buffer[*idx] = *prob;
                sum += *prob;
            }

            if sum > 0.0 {
                for value in &mut self.sampling_prob_buffer {
                    *value /= sum;
                }
            }

            result.push(self.sample_from_probs(&self.sampling_prob_buffer));
        }

        result
    }

    /// Top-p (nucleus) sampling: consider the smallest set of tokens whose cumulative probability
    /// exceeds p
    /// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨éƒ¨åˆ†æ’åºé¿å…å…¨é‡æ’åº
    fn top_p_sampling(&mut self, probs: &Array2<f32>, p: f32) -> Vec<usize> {
        let mut result = Vec::with_capacity(probs.nrows());
        let target_p = p.clamp(SOFTMAX_EPSILON, 1.0);

        for row in probs.rows() {
            let mut heap = BinaryHeap::new();
            let mut best_entry: Option<ProbEntry> = None;

            for (idx, &prob) in row.iter().enumerate() {
                if prob.is_nan() {
                    continue;
                }
                let entry = ProbEntry { prob, idx };
                if best_entry.as_ref().map_or(true, |best| entry > *best) {
                    best_entry = Some(entry.clone());
                }
                if prob > 0.0 {
                    heap.push(entry);
                }
            }

            self.sampling_prob_buffer.clear();
            self.sampling_prob_buffer
                .resize(self.vocab.words.len(), 0.0);

            let mut cumulative = 0.0;
            let mut selected_entries: Vec<ProbEntry> = Vec::new();

            while cumulative < target_p {
                match heap.pop() {
                    Some(entry) => {
                        cumulative += entry.prob;
                        selected_entries.push(entry);
                    }
                    None => break,
                }
            }

            if selected_entries.is_empty() {
                if let Some(entry) = best_entry {
                    selected_entries.push(entry);
                }
            }

            let mut sum = 0.0;
            for entry in &selected_entries {
                self.sampling_prob_buffer[entry.idx] = entry.prob;
                sum += entry.prob;
            }

            if sum > 0.0 {
                for value in &mut self.sampling_prob_buffer {
                    *value /= sum;
                }
            }

            result.push(self.sample_from_probs(&self.sampling_prob_buffer));
        }

        result
    }

    /// Sample from a probability distribution
    fn sample_from_probs(&self, probs: &[f32]) -> usize {
        let mut rng = rng();
        let sum: f32 = probs.iter().sum();

        if sum == 0.0 {
            return rng.random_range(0..probs.len());
        }

        let mut normalized_probs = Vec::new();
        let mut cumsum = 0.0;

        for &prob in probs {
            cumsum += prob / sum;
            normalized_probs.push(cumsum);
        }

        let rand_val: f32 = rng.random();
        for (i, &cum_prob) in normalized_probs.iter().enumerate() {
            if rand_val <= cum_prob {
                return i;
            }
        }

        probs.len() - 1
    }

    fn generate_tokens_full(
        &mut self,
        prompt_tokens: &[usize],
        max_new_tokens: usize,
        temperature: f32,
        top_p: f32,
        top_k: usize,
    ) -> Vec<usize> {
        if prompt_tokens.is_empty() || max_new_tokens == 0 {
            return Vec::new();
        }

        let mut perf_monitor = PerformanceMonitor::new();
        perf_monitor.start("inference_generation");
        let generation_start = Instant::now();

        let mut tokenized = prompt_tokens.to_vec();
        let mut output_tokens = Vec::with_capacity(max_new_tokens);
        let eos_id = self.vocab.eos_token_id();

        for _ in 0..max_new_tokens {
            if tokenized.len() >= self.max_context_length {
                break;
            }

            let input_vec: Vec<f32> = tokenized.iter().map(|&x| x as f32).collect();
            let token_input = match Array2::from_shape_vec((1, tokenized.len()), input_vec) {
                Ok(matrix) => matrix,
                Err(err) => {
                    log::error!("æ„é€ è¾“å…¥å¼ é‡å¤±è´¥: {}", err);
                    break;
                }
            };

            let mut input = token_input;
            for layer in &mut self.network {
                input = layer.forward(&input);
            }

            if input.shape()[0] == 0 {
                break;
            }

            let last_logit = input
                .row(input.shape()[0] - 1)
                .to_owned()
                .insert_axis(Axis(0));

            let probs = softmax(&last_logit);
            let adjusted_probs = Self::apply_temperature(&probs, temperature);

            let next_token = if top_k > 0 {
                self.top_k_sampling(&adjusted_probs, top_k)
                    .into_iter()
                    .next()
                    .unwrap_or(0)
            } else {
                self.top_p_sampling(&adjusted_probs, top_p)
                    .into_iter()
                    .next()
                    .unwrap_or(0)
            };

            output_tokens.push(next_token);
            tokenized.push(next_token);

            if next_token == eos_id {
                break;
            }
        }

        perf_monitor.stop("inference_generation");
        let tokens_generated = output_tokens.len();
        let elapsed = generation_start.elapsed().as_secs_f32();
        if tokens_generated > 0 && elapsed > 0.0 {
            println!(
                "âš¡ æ¨ç†ååé‡: {:.2} tokens/s ({} tokens)",
                tokens_generated as f32 / elapsed,
                tokens_generated
            );
        }

        output_tokens
    }

    fn generate_tokens_incremental(
        &mut self,
        prompt_tokens: &[usize],
        max_new_tokens: usize,
        temperature: f32,
        top_p: f32,
        top_k: usize,
    ) -> Vec<usize> {
        if prompt_tokens.is_empty() || max_new_tokens == 0 {
            return Vec::new();
        }

        let max_context = self.max_context_length;
        let prompt_len = prompt_tokens.len();
        let start_idx = prompt_len.saturating_sub(max_context);
        let trimmed_prompt = &prompt_tokens[start_idx..];

        if trimmed_prompt.is_empty() {
            return Vec::new();
        }

        let max_allowed_by_context = max_context.saturating_sub(trimmed_prompt.len());
        if max_allowed_by_context == 0 {
            return Vec::new();
        }
        let generation_limit = max_new_tokens.min(max_allowed_by_context);
        if generation_limit == 0 {
            return Vec::new();
        }

        if !self.supports_incremental() {
            let mut tokens = self.generate_tokens_full(
                trimmed_prompt,
                generation_limit,
                temperature,
                top_p,
                top_k,
            );
            let eos_id = self.vocab.eos_token_id();
            if !tokens.iter().any(|&token| token == eos_id) {
                tokens.push(eos_id);
            }
            return tokens;
        }

        let mut perf_monitor = PerformanceMonitor::new();
        perf_monitor.start("inference_generation");
        let generation_start = Instant::now();

        let eos_id = self.vocab.eos_token_id();
        let mut session = InferenceSession::new(self, temperature, top_p, top_k);
        let mut logits = match session.prime_tokens(trimmed_prompt) {
            Some(logits) => logits,
            None => {
                perf_monitor.stop("inference_generation");
                drop(session);
                return Vec::new();
            }
        };

        let mut generated_tokens = Vec::with_capacity(generation_limit);

        for _ in 0..generation_limit {
            let next_token = session.sample_next_token(&logits);
            generated_tokens.push(next_token);

            logits = session.advance_with_token(next_token);
            if next_token == eos_id {
                break;
            }
        }

        perf_monitor.stop("inference_generation");

        let tokens_generated = generated_tokens.len();
        let elapsed = generation_start.elapsed().as_secs_f32();
        if tokens_generated > 0 && elapsed > 0.0 {
            println!(
                "âš¡ æ¨ç†ååé‡: {:.2} tokens/s ({} tokens)",
                tokens_generated as f32 / elapsed,
                tokens_generated
            );
        }

        generated_tokens
    }

    fn beam_search_legacy(&mut self, text: &str, beam_width: usize, max_length: usize) -> String {
        if beam_width == 0 {
            return String::new();
        }

        let initial_tokens = self.tokenize(text);
        if initial_tokens.is_empty() {
            return String::new();
        }

        let mut current_beams = vec![(initial_tokens.clone(), 0.0f32)];

        for _ in initial_tokens.len()..max_length {
            self.beam_candidates_buffer.clear();

            for (seq, log_prob) in &current_beams {
                let input = match Array2::from_shape_vec(
                    (1, seq.len()),
                    seq.iter().map(|&x| x as f32).collect(),
                ) {
                    Ok(matrix) => matrix,
                    Err(err) => {
                        log::error!("æ„é€ è¾“å…¥å¼ é‡å¤±è´¥: {}", err);
                        continue;
                    }
                };

                let mut input_tensor = input;
                for layer in &mut self.network {
                    input_tensor = layer.forward(&input_tensor);
                }

                let probs = softmax(&input_tensor);
                let last_token_probs = probs.row(probs.nrows() - 1);

                self.sampling_idx_buffer.clear();
                self.sampling_idx_buffer.extend(
                    last_token_probs
                        .iter()
                        .enumerate()
                        .map(|(idx, &prob)| (prob, idx)),
                );

                self.sampling_idx_buffer
                    .sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(Ordering::Equal));

                for i in 0..beam_width.min(self.sampling_idx_buffer.len()) {
                    let (prob, token_id) = self.sampling_idx_buffer[i];
                    if prob > 0.0 {
                        let mut new_seq = seq.clone();
                        new_seq.push(token_id);
                        let new_log_prob = log_prob + prob.ln();
                        self.beam_candidates_buffer.push((new_seq, new_log_prob));
                    }
                }
            }

            if self.beam_candidates_buffer.is_empty() {
                break;
            }

            self.beam_candidates_buffer
                .sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(Ordering::Equal));
            current_beams = self
                .beam_candidates_buffer
                .iter()
                .take(beam_width)
                .cloned()
                .collect();

            if current_beams
                .iter()
                .any(|(seq, _)| seq.last() == Some(&self.vocab.eos_token_id()))
            {
                break;
            }
        }

        if let Some((best_seq, _)) = current_beams
            .iter()
            .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(Ordering::Equal))
        {
            self.tokens_to_text(best_seq)
        } else {
            String::new()
        }
    }

    /// Beam search implementation
    /// ä½¿ç”¨æ¨ç†ä¼šè¯ä¸ KV ç¼“å­˜é¿å…é‡å¤è®¡ç®—
    fn beam_search(&mut self, text: &str, beam_width: usize, max_length: usize) -> String {
        if !self.supports_incremental() {
            return self.beam_search_legacy(text, beam_width, max_length);
        }

        if beam_width == 0 {
            return String::new();
        }

        let initial_tokens = self.tokenize(text);
        if initial_tokens.is_empty() {
            return String::new();
        }

        let max_context = self.max_context_length;
        let prompt_len = initial_tokens.len();
        let start_idx = prompt_len.saturating_sub(max_context);
        let trimmed_prompt = &initial_tokens[start_idx..];

        let prompt_base_len = trimmed_prompt.len();
        let target_length = max_length.max(prompt_base_len).min(self.max_context_length);

        if target_length <= prompt_base_len {
            return self.tokens_to_text(trimmed_prompt);
        }

        #[derive(Clone)]
        struct BeamCandidate {
            tokens: Vec<usize>,
            log_prob: f32,
            state: SessionSnapshot,
            logits: Array2<f32>,
        }

        let mut perf_monitor = PerformanceMonitor::new();
        perf_monitor.start("inference_generation");
        let generation_start = Instant::now();

        let eos_id = self.vocab.eos_token_id();
        let mut session = InferenceSession::new(self, 1.0, 1.0, 0);
        let initial_logits = match session.prime_tokens(trimmed_prompt) {
            Some(logits) => logits,
            None => {
                perf_monitor.stop("inference_generation");
                drop(session);
                return self.tokens_to_text(trimmed_prompt);
            }
        };

        let mut beams = vec![BeamCandidate {
            tokens: trimmed_prompt.to_vec(),
            log_prob: 0.0,
            state: session.snapshot(),
            logits: initial_logits,
        }];

        let max_steps = target_length - trimmed_prompt.len();
        let mut buffer: Vec<(f32, usize)> = Vec::new();

        for _ in 0..max_steps {
            let mut expanded: Vec<BeamCandidate> = Vec::new();

            for candidate in &beams {
                let is_complete = candidate.tokens.len() >= target_length
                    || candidate.tokens.last() == Some(&eos_id);
                if is_complete {
                    expanded.push(candidate.clone());
                    continue;
                }

                session.restore(&candidate.state);
                let probs = softmax(&candidate.logits);
                let last_row = probs.row(probs.nrows() - 1);

                buffer.clear();
                buffer.extend(last_row.iter().enumerate().map(|(idx, &prob)| (prob, idx)));

                let top = beam_width.min(buffer.len());
                if top == 0 {
                    expanded.push(candidate.clone());
                    continue;
                }

                let nth = top - 1;
                buffer.select_nth_unstable_by(nth, |a, b| {
                    b.0.partial_cmp(&a.0).unwrap_or(Ordering::Equal)
                });
                buffer[..top]
                    .sort_unstable_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(Ordering::Equal));

                for &(prob, token_idx) in buffer[..top].iter() {
                    if prob <= 0.0 {
                        continue;
                    }

                    let log_prob = candidate.log_prob + prob.max(SOFTMAX_EPSILON).ln();

                    session.restore(&candidate.state);
                    let next_logits = session.advance_with_token(token_idx);
                    let snapshot = session.snapshot();

                    let mut new_tokens = candidate.tokens.clone();
                    new_tokens.push(token_idx);

                    expanded.push(BeamCandidate {
                        tokens: new_tokens,
                        log_prob,
                        state: snapshot,
                        logits: next_logits,
                    });
                }
            }

            if expanded.is_empty() {
                break;
            }

            expanded.sort_by(|a, b| {
                b.log_prob
                    .partial_cmp(&a.log_prob)
                    .unwrap_or(Ordering::Equal)
            });
            beams = expanded.into_iter().take(beam_width).collect();

            let all_complete = beams.iter().all(|candidate| {
                candidate.tokens.len() >= target_length || candidate.tokens.last() == Some(&eos_id)
            });
            if all_complete {
                break;
            }
        }

        perf_monitor.stop("inference_generation");

        let best_candidate = beams
            .iter()
            .max_by(|a, b| {
                a.log_prob
                    .partial_cmp(&b.log_prob)
                    .unwrap_or(Ordering::Equal)
            })
            .cloned();

        drop(session);

        if let Some(best) = best_candidate {
            let generated_tokens = best.tokens.len().saturating_sub(prompt_base_len);
            let elapsed = generation_start.elapsed().as_secs_f32();
            if generated_tokens > 0 && elapsed > 0.0 {
                println!(
                    "âš¡ æ¨ç†ååé‡: {:.2} tokens/s ({} tokens)",
                    generated_tokens as f32 / elapsed,
                    generated_tokens
                );
            }
            self.tokens_to_text(&best.tokens)
        } else {
            String::new()
        }
    }

    fn tokens_to_text(&self, tokens: &[usize]) -> String {
        if tokens.is_empty() {
            return String::new();
        }

        let token_strs: Vec<String> = tokens
            .iter()
            .filter_map(|&token| self.vocab.decode.get(&token).cloned())
            .collect();

        let raw_output = token_strs.join(" ");
        self.post_process_chinese_text(&raw_output)
    }

    /// Post-process generated Chinese text to improve fluency and accuracy
    pub fn post_process_chinese_text(&self, text: &str) -> String {
        // Remove extra spaces between Chinese characters
        let mut result = String::new();
        let mut chars = text.chars().peekable();

        while let Some(ch) = chars.next() {
            result.push(ch);

            // If current and next characters are both Chinese, don't add space
            if let Some(&next_ch) = chars.peek() {
                if self.is_chinese_char(ch) && self.is_chinese_char(next_ch) {
                    // Skip any space between Chinese characters
                    if next_ch == ' ' {
                        chars.next(); // consume the space
                    }
                }
            }
        }

        // Additional processing could include:
        // - Grammar pattern correction
        // - Ensuring proper sentence structure
        // - Removing repetitive patterns

        result
    }

    /// Helper function to check if a character is Chinese
    fn is_chinese_char(&self, ch: char) -> bool {
        (ch as u32) >= 0x4E00 && (ch as u32) <= 0x9FFF
    }

    fn cross_entropy_from_log_probs(log_probs: &Array2<f32>, target: &[usize]) -> f32 {
        // ä½¿ç”¨ log_softmax è¾“å‡ºè®¡ç®—äº¤å‰ç†µï¼Œé¿å…å¯¹æ¦‚ç‡å–å¯¹æ•°çš„æ•°å€¼ä¸ç¨³å®š
        let mut loss = 0.0;
        let n_targets = target.len() as f32;

        for (row_idx, &target_idx) in target.iter().enumerate() {
            let lp = log_probs[[row_idx, target_idx]];
            loss -= lp; // NLL: -log p(target)
        }

        loss / n_targets
    }

    /// è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆä»softmaxæ¦‚ç‡ï¼‰
    pub fn cross_entropy_loss_step(probs: &Array2<f32>, target: &[usize]) -> f32 {
        use crate::LOG_EPSILON;
        let mut loss = 0.0;
        let n_targets = target.len() as f32;

        for (row_idx, &target_idx) in target.iter().enumerate() {
            if target_idx < probs.shape()[1] {
                let prob = probs[[row_idx, target_idx]].max(LOG_EPSILON);
                loss -= prob.ln();
            }
        }

        loss / n_targets
    }

    pub fn compute_gradients_step(probs: &Array2<f32>, target: &[usize]) -> Array2<f32> {
        let mut grads = probs.clone(); // softmax - one_hot(target)

        if probs.shape()[0] != target.len() {
            log::error!(
                "æ¢¯åº¦è®¡ç®—è¾“å…¥ä¸åŒ¹é…ï¼šprobsè¡Œæ•°={}ï¼Œtargeté•¿åº¦={}",
                probs.shape()[0],
                target.len()
            );
            return grads; // è¿”å›åŸå§‹æ¢¯åº¦ï¼Œé¿å…å´©æºƒ
        }

        let batch_size = target.len() as f32;

        for (row_idx, &target_idx) in target.iter().enumerate() {
            grads[[row_idx, target_idx]] -= 1.0;
        }

        grads.mapv_inplace(|x| x / batch_size);
        grads
    }

    pub fn clip_gradients(grads: &mut Array2<f32>, max_norm: f32) {
        // è®¡ç®—L2èŒƒæ•°å¹¶è£å‰ª
        let norm = grads.iter().map(|&x| x * x).sum::<f32>().sqrt();
        if norm > max_norm {
            let scale = max_norm / norm;
            grads.mapv_inplace(|x| x * scale);
        }
    }

    fn apply_accumulated_gradients(
        &mut self,
        gradient_bucket: &mut Vec<Array2<f32>>,
        current_lr: f32,
        use_parallel: bool,
        perf_monitor: &mut PerformanceMonitor,
    ) {
        if gradient_bucket.is_empty() {
            return;
        }

        let label = if use_parallel && gradient_bucket.len() > 1 {
            "æ¢¯åº¦ç´¯ç§¯(å¹¶è¡Œå½’çº¦)"
        } else {
            "æ¢¯åº¦ç´¯ç§¯(å•çº¿ç¨‹å½’çº¦)"
        };

        let track_perf = use_parallel && gradient_bucket.len() > 1;

        if track_perf {
            perf_monitor.start(label);
        }

        let aggregated = if use_parallel && gradient_bucket.len() > 1 {
            Self::aggregate_gradients_parallel(gradient_bucket.as_slice())
        } else {
            Self::aggregate_gradients_sequential(gradient_bucket.as_slice())
        };

        if track_perf {
            perf_monitor.stop(label);
        }

        gradient_bucket.clear();

        let mut current_grad = aggregated;
        for layer in self.network.iter_mut().rev() {
            current_grad = layer.backward(&current_grad, current_lr);
        }
    }

    fn aggregate_gradients_sequential(gradients: &[Array2<f32>]) -> Array2<f32> {
        if gradients.is_empty() {
            return Array2::zeros((0, 0));
        }

        let mut acc = gradients[0].clone();
        for grad in &gradients[1..] {
            acc += grad;
        }
        acc.mapv_inplace(|x| x / gradients.len() as f32);
        acc
    }

    fn aggregate_gradients_parallel(gradients: &[Array2<f32>]) -> Array2<f32> {
        // ç®€åŒ–å®ç°ï¼šå¯¹äºå°æ¨¡å‹ï¼Œä¸²è¡Œèšåˆæ€§èƒ½è¶³å¤Ÿå¥½
        Self::aggregate_gradients_sequential(gradients)
    }
}
