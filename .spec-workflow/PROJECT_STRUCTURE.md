# é¡¹ç›®ç»“æ„æ–‡æ¡£ - RustGPT-Chinese

## ğŸ“ ç›®å½•ç»“æ„æ€»è§ˆ

```
RustGPT-Chinese/
â”œâ”€â”€ .github/                    # GitHub é…ç½®
â”‚   â””â”€â”€ workflows/              # CI/CD å·¥ä½œæµ
â”‚       â”œâ”€â”€ check.yml           # ä»£ç è´¨é‡æ£€æŸ¥ï¼ˆfmt + clippyï¼‰
â”‚       â””â”€â”€ test.yml            # è‡ªåŠ¨åŒ–æµ‹è¯•
â”‚
â”œâ”€â”€ .spec-workflow/             # é¡¹ç›®è§„èŒƒæ–‡æ¡£ï¼ˆæœ¬ç›®å½•ï¼‰
â”‚   â”œâ”€â”€ SPEC_WORKFLOW.md        # å®Œæ•´è§„èŒƒå’Œå·¥ä½œæµç¨‹
â”‚   â”œâ”€â”€ TECH_STACK.md           # æŠ€æœ¯æ ˆè¯¦ç»†è¯´æ˜
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md    # æœ¬æ–‡æ¡£
â”‚   â”œâ”€â”€ templates/              # Spec-workflow æ¨¡æ¿
â”‚   â””â”€â”€ config.example.toml     # é…ç½®ç¤ºä¾‹
â”‚
â”œâ”€â”€ .claude/                    # Claude AI è¾…åŠ©å¼€å‘é…ç½®
â”‚   â””â”€â”€ output-styles/          # è¾“å‡ºé£æ ¼å®šä¹‰
â”‚
â”œâ”€â”€ benches/                    # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ memory_optimization_bench.rs  # å†…å­˜ä¼˜åŒ–åŸºå‡†
â”‚   â””â”€â”€ performance_benchmark.rs      # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚
â”œâ”€â”€ data/                       # è®­ç»ƒæ•°æ®å’Œèµ„æº
â”‚   â”œâ”€â”€ pretraining_data.json   # é¢„è®­ç»ƒæ•°æ®ï¼ˆä¸­æ–‡çŸ¥è¯†ï¼‰
â”‚   â”œâ”€â”€ chat_training_data.json # å¯¹è¯è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ chinese_idioms.json     # ä¸­æ–‡æˆè¯­è¯å…¸
â”‚
â”œâ”€â”€ examples/                   # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ (æœªæ¥æ·»åŠ æ•™å­¦ç¤ºä¾‹)
â”‚
â”œâ”€â”€ src/                        # æºä»£ç ï¼ˆæ ¸å¿ƒå®ç°ï¼‰
â”‚   â”œâ”€â”€ lib.rs                  # åº“å…¥å£ + å…¨å±€é…ç½®
â”‚   â”œâ”€â”€ main.rs                 # å‘½ä»¤è¡Œä¸»ç¨‹åº
â”‚   â”‚
â”‚   â”œâ”€â”€ llm.rs                  # LLM æ ¸å¿ƒç±»ï¼ˆå‰å‘/åå‘ä¼ æ’­ï¼‰
â”‚   â”œâ”€â”€ vocab.rs                # è¯æ±‡è¡¨å’Œåˆ†è¯
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings.rs           # Token åµŒå…¥å±‚
â”‚   â”œâ”€â”€ self_attention.rs       # å¤šå¤´è‡ªæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ feed_forward.rs         # å‰é¦ˆç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ layer_norm.rs           # å±‚å½’ä¸€åŒ–
â”‚   â”œâ”€â”€ dropout.rs              # Dropout æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ output_projection.rs    # è¾“å‡ºæŠ•å½±å±‚
â”‚   â”œâ”€â”€ transformer.rs          # Transformer Block
â”‚   â”‚
â”‚   â”œâ”€â”€ adam.rs                 # Adam ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ training_optimizations.rs  # è®­ç»ƒä¼˜åŒ–ï¼ˆå­¦ä¹ ç‡ã€æ—©åœï¼‰
â”‚   â”œâ”€â”€ batch_loader.rs         # æ‰¹é‡æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ checkpoint_manager.rs   # æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ fused_ops.rs            # èåˆç®—å­ä¼˜åŒ–
â”‚   â”œâ”€â”€ position_encoding.rs    # ä½ç½®ç¼–ç 
â”‚   â”œâ”€â”€ performance_monitor.rs  # æ€§èƒ½ç›‘æ§
â”‚   â”‚
â”‚   â”œâ”€â”€ dataset_loader.rs       # æ•°æ®åŠ è½½å·¥å…·
â”‚   â”œâ”€â”€ model_serialization.rs  # æ¨¡å‹åºåˆ—åŒ–
â”‚   â””â”€â”€ utils.rs                # é€šç”¨å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ tests/                      # é›†æˆå’Œå•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ llm_test.rs             # LLM é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ transformer_test.rs     # Transformer Block æµ‹è¯•
â”‚   â”œâ”€â”€ self_attention_test.rs  # è‡ªæ³¨æ„åŠ›æµ‹è¯•
â”‚   â”œâ”€â”€ feed_forward_test.rs    # å‰é¦ˆç½‘ç»œæµ‹è¯•
â”‚   â”œâ”€â”€ embeddings_test.rs      # åµŒå…¥å±‚æµ‹è¯•
â”‚   â”œâ”€â”€ output_projection_test.rs  # è¾“å‡ºå±‚æµ‹è¯•
â”‚   â”œâ”€â”€ adam_test.rs            # ä¼˜åŒ–å™¨æµ‹è¯•
â”‚   â”œâ”€â”€ dataset_loader_test.rs  # æ•°æ®åŠ è½½æµ‹è¯•
â”‚   â”œâ”€â”€ position_encoding_test.rs  # ä½ç½®ç¼–ç æµ‹è¯•
â”‚   â”œâ”€â”€ vocab_test.rs           # è¯æ±‡è¡¨æµ‹è¯•
â”‚   â””â”€â”€ chinese_tests.rs        # ä¸­æ–‡å¤„ç†æµ‹è¯•
â”‚
â”œâ”€â”€ Cargo.toml                  # Rust é¡¹ç›®é…ç½®
â”œâ”€â”€ Cargo.lock                  # ä¾èµ–é”å®šæ–‡ä»¶
â”œâ”€â”€ rustfmt.toml                # ä»£ç æ ¼å¼é…ç½®
â”œâ”€â”€ .gitignore                  # Git å¿½ç•¥è§„åˆ™
â”‚
â”œâ”€â”€ LICENSE.txt                 # MIT å¼€æºè®¸å¯è¯
â”œâ”€â”€ README.md                   # é¡¹ç›®è¯´æ˜ï¼ˆè‹±æ–‡ï¼‰
â”œâ”€â”€ README_zh.md                # é¡¹ç›®è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰
â”œâ”€â”€ CLAUDE.md                   # AI å¼€å‘æŒ‡å—
â”œâ”€â”€ BATCH_TRAINING.md           # æ‰¹é‡è®­ç»ƒæ–‡æ¡£
â”œâ”€â”€ PERFORMANCE_OPTIMIZATIONS.md  # æ€§èƒ½ä¼˜åŒ–è¯´æ˜
â””â”€â”€ IMPLEMENTATION_v0.4.0.md    # ç‰ˆæœ¬å®ç°ç¬”è®°
```

---

## ğŸ“„ æ ¸å¿ƒæ–‡ä»¶è¯¦è§£

### 1. Cargo.toml - é¡¹ç›®æ¸…å•

```toml
[package]
name = "llm"              # åŒ…å
version = "0.4.0"         # å½“å‰ç‰ˆæœ¬
edition = "2024"          # Rust 2024 ç‰ˆæœ¬

[dependencies]
# æ ¸å¿ƒä¾èµ–ï¼ˆè¯¦è§ TECH_STACK.mdï¼‰
ndarray = "0.16.1"        # å¼ é‡è®¡ç®—
jieba-rs = "0.7"          # ä¸­æ–‡åˆ†è¯
lru = "0.12"              # LRU ç¼“å­˜
# ... (å…¶ä»–ä¾èµ–)

[features]
default = []              # é»˜è®¤æ— ç‰¹æ®Šç‰¹æ€§
blas = ["dep:blas-src", "dep:openblas-src", "ndarray/blas"]

[profile.release]
opt-level = 3             # æœ€é«˜ä¼˜åŒ–
lto = true                # é“¾æ¥æ—¶ä¼˜åŒ–
codegen-units = 1         # å•ä»£ç ç”Ÿæˆå•å…ƒ
strip = true              # ç§»é™¤è°ƒè¯•ç¬¦å·
```

**å…³é”®é…ç½®é¡¹**:
- `edition = "2024"` - ä½¿ç”¨æœ€æ–° Rust ç‰¹æ€§
- `features` - BLAS åŠ é€Ÿä¸ºå¯é€‰ç‰¹æ€§
- `[lib]` å’Œ `[[bin]]` - æ”¯æŒåº“å’Œå¯æ‰§è¡Œæ–‡ä»¶åŒæ¨¡å¼

### 2. src/lib.rs - å…¨å±€é…ç½®å’Œå…¬å…±æ¥å£

**èŒè´£**:
- å®šä¹‰å…¨å±€å¸¸é‡ï¼ˆæ¨¡å‹è¶…å‚æ•°ï¼‰
- å£°æ˜å…¬å…±æ¨¡å—å’Œå¯¼å‡º
- å®šä¹‰ `Layer` traitï¼ˆæ‰€æœ‰ç¥ç»ç½‘ç»œå±‚çš„ç»Ÿä¸€æ¥å£ï¼‰

**å…³é”®ä»£ç **:
```rust
// å…¨å±€é…ç½®ï¼ˆæ•™è‚²å‹å¥½çš„å°æ¨¡å‹å‚æ•°ï¼‰
pub const MAX_SEQ_LEN: usize = 128;      // åºåˆ—æœ€å¤§é•¿åº¦
pub const EMBEDDING_DIM: usize = 256;    // åµŒå…¥ç»´åº¦
pub const HIDDEN_DIM: usize = 512;       // å‰é¦ˆéšè—å±‚ç»´åº¦
pub const NUM_HEADS: usize = 8;          // æ³¨æ„åŠ›å¤´æ•°
pub const NUM_LAYERS: usize = 2;         // Transformer å±‚æ•°
pub const VOCAB_SIZE: usize = 30000;     // è¯æ±‡è¡¨ç›®æ ‡å¤§å°
pub const DROPOUT_RATE: f32 = 0.1;       // Dropout æ¯”ç‡

// Layer traitï¼ˆç»Ÿä¸€æ¥å£ï¼‰
pub trait Layer: Send + Sync {
    fn layer_type(&self) -> &str;
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32>;
    fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32>;
    fn parameters(&self) -> usize;
    fn set_training_mode(&mut self, training: bool);
}

// æ¨¡å—å¯¼å‡º
pub mod llm;
pub mod vocab;
pub mod embeddings;
pub mod self_attention;
// ... (å…¶ä»–æ¨¡å—)
```

### 3. src/main.rs - ä¸»ç¨‹åºå…¥å£

**èŒè´£**:
- å‘½ä»¤è¡Œäº¤äº’ç•Œé¢
- è®­ç»ƒæµç¨‹ç¼–æ’ï¼ˆé¢„è®­ç»ƒ + æŒ‡ä»¤å¾®è°ƒï¼‰
- äº¤äº’å¼æ¨ç†æ¨¡å¼
- æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§

**æ ¸å¿ƒæµç¨‹**:
```rust
fn main() {
    simple_logger::init().unwrap();
    
    // 1. åŠ è½½è®­ç»ƒæ•°æ®
    let pretraining_data = load_json_data("data/pretraining_data.json");
    let chat_data = load_json_data("data/chat_training_data.json");
    
    // 2. æ„å»ºè¯æ±‡è¡¨ï¼ˆåˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ï¼‰
    let vocab = Vocab::build_from_texts(&all_texts);
    
    // 3. é¢„è®­ç»ƒé˜¶æ®µï¼ˆå­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ï¼‰
    let mut model = LLM::new(vocab.clone());
    model.train_monitored(&pretraining_data, 500, 0.001, "é¢„è®­ç»ƒ");
    
    // 4. æŒ‡ä»¤å¾®è°ƒï¼ˆå­¦ä¹ å¯¹è¯æ¨¡å¼ï¼‰
    model.train_monitored(&chat_data, 500, 0.0005, "æŒ‡ä»¤å¾®è°ƒ");
    
    // 5. ä¿å­˜æ¨¡å‹
    model.save_to_file("model_checkpoint.bin");
    
    // 6. äº¤äº’å¼æ¨ç†
    interactive_mode(&mut model);
}
```

**äº¤äº’æ¨¡å¼ç¤ºä¾‹**:
```
è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰: ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ
[Beam Search, width=3, max_len=20]
æ¨¡å‹å›ç­”: æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ...
```

---

## ğŸ§  æ ¸å¿ƒæ¨¡å—è¯¦è§£

### Neural Network Layersï¼ˆç¥ç»ç½‘ç»œå±‚ï¼‰

#### src/embeddings.rs - åµŒå…¥å±‚

**åŠŸèƒ½**:
- Token åµŒå…¥ï¼šå°† token ID æ˜ å°„åˆ° 256 ç»´å‘é‡
- ä½ç½®ç¼–ç ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆä½¿ç”¨ `position_encoding.rs`ï¼‰

**å…³é”®æ–¹æ³•**:
```rust
pub struct Embeddings {
    token_embed: Array2<f32>,     // (vocab_size, embedding_dim)
    position_encoder: PositionEncoding,
}

impl Layer for Embeddings {
    fn forward(&mut self, token_ids: &Array2<f32>) -> Array2<f32> {
        // 1. Token åµŒå…¥æŸ¥æ‰¾
        // 2. æ·»åŠ ä½ç½®ç¼–ç 
        // 3. è¿”å› (batch_size, seq_len, embedding_dim)
    }
    
    fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32> {
        // æ›´æ–° token_embed æƒé‡
    }
}
```

**è¾“å…¥/è¾“å‡º**:
- è¾“å…¥: Token IDs `(seq_len, 1)` - æ•´æ•°æ•°ç»„
- è¾“å‡º: åµŒå…¥å‘é‡ `(seq_len, 256)` - æµ®ç‚¹æ•°å¼ é‡

#### src/self_attention.rs - å¤šå¤´è‡ªæ³¨æ„åŠ›

**åŠŸèƒ½**:
- å®ç° Scaled Dot-Product Attention
- å¤šå¤´æœºåˆ¶ï¼ˆ8 ä¸ªæ³¨æ„åŠ›å¤´ï¼‰
- KV-Cache æ”¯æŒï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰

**ç®—æ³•**:
```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V
MultiHead = Concat(head_1, ..., head_8) @ W_o
```

**å…³é”®æ–¹æ³•**:
```rust
pub struct SelfAttention {
    num_heads: usize,              // 8
    d_model: usize,                // 256
    d_k: usize,                    // 32 (256/8)
    
    w_q: Array2<f32>,              // Query æƒé‡
    w_k: Array2<f32>,              // Key æƒé‡
    w_v: Array2<f32>,              // Value æƒé‡
    w_o: Array2<f32>,              // è¾“å‡ºæƒé‡
    
    kv_cache_enabled: bool,        // KV-Cache å¼€å…³
    cached_keys: Vec<Array2<f32>>,
    cached_values: Vec<Array2<f32>>,
}

impl Layer for SelfAttention {
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
        // 1. çº¿æ€§æŠ•å½±: Q = X @ W_q, K = X @ W_k, V = X @ W_v
        // 2. åˆ†å‰²å¤šå¤´
        // 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: scores = Q @ K^T / âˆšd_k
        // 4. Softmax å½’ä¸€åŒ–
        // 5. åŠ æƒæ±‚å’Œ: output = softmax(scores) @ V
        // 6. æ‹¼æ¥å¤šå¤´ + è¾“å‡ºæŠ•å½±
    }
    
    fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32> {
        // åå‘ä¼ æ’­æ›´æ–° W_q, W_k, W_v, W_o
    }
}
```

**ç‰¹æ®Šæ–¹æ³•**:
```rust
pub fn enable_kv_cache(&mut self);    // å¯ç”¨ KV-Cacheï¼ˆæ¨ç†åŠ é€Ÿï¼‰
pub fn clear_kv_cache(&mut self);     // æ¸…é™¤ç¼“å­˜ï¼ˆæ–°ä¼šè¯ï¼‰
```

#### src/feed_forward.rs - å‰é¦ˆç¥ç»ç½‘ç»œ

**åŠŸèƒ½**:
- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
- GELU æ¿€æ´»å‡½æ•°ï¼ˆå¹³æ»‘ç‰ˆ ReLUï¼‰
- æ®‹å·®è¿æ¥å’Œ Dropout

**æ¶æ„**:
```
Input (256d) â†’ Linear (256 â†’ 512) â†’ GELU â†’ Linear (512 â†’ 256) â†’ Output (256d)
```

**å…³é”®ä»£ç **:
```rust
pub struct FeedForward {
    w1: Array2<f32>,              // (256, 512)
    b1: Array1<f32>,              // (512,)
    w2: Array2<f32>,              // (512, 256)
    b2: Array1<f32>,              // (256,)
}

fn gelu(x: f32) -> f32 {
    // GELU(x) = x * Î¦(x)ï¼Œå…¶ä¸­ Î¦ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
    // è¿‘ä¼¼: 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))
}
```

#### src/layer_norm.rs - å±‚å½’ä¸€åŒ–

**åŠŸèƒ½**:
- å½’ä¸€åŒ–ç‰¹å¾ç»´åº¦ï¼ˆé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼‰
- å¯å­¦ä¹ çš„ scale å’Œ shift å‚æ•°

**ç®—æ³•**:
```
Î¼ = mean(x)
ÏƒÂ² = variance(x)
normalized = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)
output = Î³ * normalized + Î²
```

**å…³é”®å‚æ•°**:
```rust
pub struct LayerNorm {
    gamma: Array1<f32>,    // Scale å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
    beta: Array1<f32>,     // Shift å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
    epsilon: f32,          // æ•°å€¼ç¨³å®šæ€§ï¼ˆ1e-5ï¼‰
}
```

#### src/dropout.rs - Dropout æ­£åˆ™åŒ–

**åŠŸèƒ½**:
- è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ 10% çš„ç¥ç»å…ƒ
- æ¨ç†æ—¶ä¸ä¸¢å¼ƒï¼ˆä¹˜ä»¥ä¿ç•™æ¦‚ç‡ï¼‰

**å®ç°**:
```rust
pub struct Dropout {
    dropout_rate: f32,           // 0.1
    training: bool,              // è®­ç»ƒ/æ¨ç†æ¨¡å¼åˆ‡æ¢
    mask: Option<Array2<f32>>,   // ç¼“å­˜çš„ Dropout æ©ç 
}

impl Layer for Dropout {
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
        if self.training {
            // ç”Ÿæˆéšæœºæ©ç : mask[i] = 1 if rand() > 0.1 else 0
            // output = input * mask / (1 - dropout_rate)
        } else {
            // æ¨ç†æ—¶ç›´æ¥è¿”å›
            input.clone()
        }
    }
}
```

#### src/output_projection.rs - è¾“å‡ºæŠ•å½±å±‚

**åŠŸèƒ½**:
- å°† Transformer è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°
- ç”¨äºä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ

**æ¶æ„**:
```
Input (256d) â†’ Linear (256 â†’ vocab_size) â†’ Softmax â†’ Probabilities
```

#### src/transformer.rs - Transformer Block

**åŠŸèƒ½**:
- ç»„åˆæ‰€æœ‰å±‚ï¼ˆSelf-Attention + FFN + LayerNorm + Dropoutï¼‰
- Pre-LN æ¶æ„ï¼ˆLayerNorm åœ¨ Attention ä¹‹å‰ï¼‰

**ç»“æ„**:
```rust
pub struct TransformerBlock {
    norm1: LayerNorm,              // ç¬¬ä¸€ä¸ª LayerNorm
    attention: SelfAttention,      // å¤šå¤´è‡ªæ³¨æ„åŠ›
    dropout1: Dropout,             // ç¬¬ä¸€ä¸ª Dropout
    
    norm2: LayerNorm,              // ç¬¬äºŒä¸ª LayerNorm
    feed_forward: FeedForward,     // å‰é¦ˆç½‘ç»œ
    dropout2: Dropout,             // ç¬¬äºŒä¸ª Dropout
}

impl Layer for TransformerBlock {
    fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
        // å­å±‚ 1: LayerNorm â†’ Self-Attention â†’ Dropout â†’ Residual
        let normed1 = self.norm1.forward(input);
        let attn_out = self.attention.forward(&normed1);
        let dropped1 = self.dropout1.forward(&attn_out);
        let residual1 = input + &dropped1;
        
        // å­å±‚ 2: LayerNorm â†’ FeedForward â†’ Dropout â†’ Residual
        let normed2 = self.norm2.forward(&residual1);
        let ffn_out = self.feed_forward.forward(&normed2);
        let dropped2 = self.dropout2.forward(&ffn_out);
        let residual2 = &residual1 + &dropped2;
        
        residual2
    }
}
```

---

### Model Orchestrationï¼ˆæ¨¡å‹ç¼–æ’ï¼‰

#### src/llm.rs - LLM æ ¸å¿ƒç±»ï¼ˆ~600 è¡Œï¼‰

**èŒè´£**: æ•´ä¸ªè¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒï¼Œè´Ÿè´£:
1. **å‰å‘ä¼ æ’­**: Embeddings â†’ Transformer Blocks â†’ Output Projection
2. **åå‘ä¼ æ’­**: é“¾å¼æ±‚å¯¼æ›´æ–°æ‰€æœ‰å±‚å‚æ•°
3. **è®­ç»ƒå¾ªç¯**: Teacher Forcing + Cross-Entropy Loss
4. **æ¨ç†ç”Ÿæˆ**: å¤šç§é‡‡æ ·ç­–ç•¥ï¼ˆGreedy, Top-K, Top-P, Beam Searchï¼‰
5. **ä¸Šä¸‹æ–‡ç®¡ç†**: å¯¹è¯å†å²ç»´æŠ¤

**å…³é”®ç»“æ„**:
```rust
pub struct LLM {
    vocab: Vocab,                           // è¯æ±‡è¡¨
    layers: Vec<Box<dyn Layer>>,            // æ‰€æœ‰ç¥ç»ç½‘ç»œå±‚
    
    context: Vec<usize>,                    // ä¸Šä¸‹æ–‡ token IDs
    optimizer: Adam,                        // Adam ä¼˜åŒ–å™¨
    
    max_context_length: usize,              // ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆ128ï¼‰
}
```

**æ ¸å¿ƒæ–¹æ³•**:
```rust
// è®­ç»ƒæ–¹æ³•ï¼ˆå¸¦ç›‘æ§ï¼‰
pub fn train_monitored(
    &mut self, 
    dataset: &[String], 
    epochs: usize, 
    base_lr: f32,
    phase_name: &str
) {
    // 1. æ•°æ®é¢„å¤„ç†å’Œç¼“å­˜
    // 2. ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
    // 3. æ—©åœæœºåˆ¶
    // 4. æ¢¯åº¦ç´¯ç§¯ï¼ˆ4 æ­¥ï¼‰
    // 5. æ€§èƒ½ç›‘æ§ï¼ˆLoss, PPL, LR, Grad, Speed, ETAï¼‰
}

// å‰å‘ä¼ æ’­
fn forward(&mut self, tokens: &[usize], training: bool) -> Array2<f32> {
    // ä¾æ¬¡è°ƒç”¨æ‰€æœ‰å±‚çš„ forward æ–¹æ³•
}

// åå‘ä¼ æ’­
fn backward(&mut self, loss_grad: &Array2<f32>, lr: f32) {
    // åå‘éå†æ‰€æœ‰å±‚ï¼Œè°ƒç”¨ backward æ–¹æ³•
}

// é‡‡æ ·æ–¹æ³•
pub fn sample_greedy(&self, logits: &Array1<f32>) -> usize;
pub fn sample_top_k(&self, logits: &Array1<f32>, k: usize) -> usize;
pub fn sample_top_p(&self, logits: &Array1<f32>, p: f32) -> usize;

// Beam Search ç”Ÿæˆ
pub fn generate_beam_search(
    &mut self, 
    prompt: &str, 
    beam_width: usize, 
    max_length: usize
) -> String;
```

**è®­ç»ƒæ•°æ®æµ**:
```
Text â†’ Tokenize â†’ Token IDs
  â†’ Embeddings (256d)
  â†’ TransformerBlock 1 (Attention + FFN)
  â†’ TransformerBlock 2 (Attention + FFN)
  â†’ OutputProjection (vocab_size)
  â†’ Softmax â†’ Probabilities
  â†’ Cross-Entropy Loss
  â†’ Backward Pass â†’ Parameter Update
```

---

### Training Infrastructureï¼ˆè®­ç»ƒåŸºç¡€è®¾æ–½ï¼‰

#### src/adam.rs - Adam ä¼˜åŒ–å™¨

**åŠŸèƒ½**:
- è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•
- ç»´æŠ¤ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆRMSPropï¼‰

**ç®—æ³•**:
```
m_t = Î²â‚ * m_{t-1} + (1 - Î²â‚) * g_t
v_t = Î²â‚‚ * v_{t-1} + (1 - Î²â‚‚) * g_tÂ²
Î¸_t = Î¸_{t-1} - Î± * m_t / (âˆšv_t + Îµ)
```

**å…³é”®å‚æ•°**:
```rust
pub struct Adam {
    beta1: f32,        // 0.9ï¼ˆä¸€é˜¶çŸ©è¡°å‡ï¼‰
    beta2: f32,        // 0.999ï¼ˆäºŒé˜¶çŸ©è¡°å‡ï¼‰
    epsilon: f32,      // 1e-8ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
    
    m: HashMap<usize, Array2<f32>>,  // ä¸€é˜¶çŸ©ï¼ˆmomentumï¼‰
    v: HashMap<usize, Array2<f32>>,  // äºŒé˜¶çŸ©ï¼ˆRMSPropï¼‰
    t: usize,          // æ—¶é—´æ­¥ï¼ˆç”¨äºåå·®ä¿®æ­£ï¼‰
}
```

#### src/training_optimizations.rs - è®­ç»ƒä¼˜åŒ–

**åŠŸèƒ½**:
1. **ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦**ï¼ˆCosine Annealing with Warm Restartsï¼‰
2. **æ—©åœæœºåˆ¶**ï¼ˆEarly Stoppingï¼‰
3. **æ¢¯åº¦ç´¯ç§¯**ï¼ˆGradient Accumulationï¼‰
4. **å®Œæ•´è®­ç»ƒç›‘æ§**ï¼ˆLoss, PPL, LR, Grad Norm, Speed, ETAï¼‰

**å…³é”®å‡½æ•°**:
```rust
// ä½™å¼¦é€€ç«å­¦ä¹ ç‡ï¼ˆå¸¦é‡å¯ï¼‰
pub fn cosine_annealing_with_restarts(
    base_lr: f32,
    epoch: usize,
    max_epochs: usize,
    num_restarts: usize
) -> f32 {
    // LR = base_lr * 0.5 * (1 + cos(Ï€ * cycle_progress))
}

// æ—©åœæ£€æŸ¥
pub struct EarlyStopping {
    patience: usize,              // 30 epochs
    best_loss: f32,
    counter: usize,
    should_stop: bool,
}

pub fn check_early_stopping(&mut self, current_loss: f32) -> bool;
```

#### src/batch_loader.rs - æ‰¹é‡æ•°æ®åŠ è½½

**åŠŸèƒ½**:
- ä¸€æ¬¡æ€§é¢„å¤„ç†æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼ˆtokenizationï¼‰
- ç¼“å­˜ token IDsï¼Œé¿å…é‡å¤è®¡ç®—
- å‡å°‘ 20-30% è®­ç»ƒæ—¶é—´

**å…³é”®æ–¹æ³•**:
```rust
pub struct BatchLoader {
    cached_token_ids: Vec<Vec<usize>>,  // ç¼“å­˜çš„ token IDs
    vocab: Vocab,
}

pub fn preprocess_all_texts(texts: &[String], vocab: &Vocab) -> BatchLoader {
    // ä¸€æ¬¡æ€§åˆ†è¯å¹¶ç¼“å­˜
}
```

#### src/checkpoint_manager.rs - æ£€æŸ¥ç‚¹ç®¡ç†

**åŠŸèƒ½**:
- å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆæ¯ 50 epochsï¼‰
- ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº lossï¼‰
- æ”¯æŒè®­ç»ƒä¸­æ–­æ¢å¤

**å…³é”®æ–¹æ³•**:
```rust
pub struct CheckpointManager {
    checkpoint_dir: String,
    best_loss: f32,
}

pub fn save_checkpoint(
    &mut self,
    model: &LLM,
    epoch: usize,
    loss: f32
) -> Result<(), String>;

pub fn load_checkpoint(path: &str) -> Result<LLM, String>;
```

---

### Data Processingï¼ˆæ•°æ®å¤„ç†ï¼‰

#### src/vocab.rs - è¯æ±‡è¡¨å’Œåˆ†è¯ï¼ˆ~1000 è¡Œï¼‰

**åŠŸèƒ½**:
1. **ä¸­æ–‡åˆ†è¯**: ä½¿ç”¨ jieba-rs
2. **ç‰¹æ®Š token ç®¡ç†**: `<|pad|>`, `<|unk|>`, `<|bos|>`, `</s>` ç­‰
3. **æˆè¯­æ£€æµ‹**: å››å­—æˆè¯­è¯†åˆ«ï¼ˆæ­£åˆ™ + è¯å…¸ï¼‰
4. **LRU ç¼“å­˜**: ç¼“å­˜åˆ†è¯ç»“æœï¼ˆ10,000 æ¡ç›®ï¼‰

**æ ¸å¿ƒç»“æ„**:
```rust
pub struct Vocab {
    token_to_id: HashMap<String, usize>,    // Token â†’ ID
    id_to_token: HashMap<usize, String>,    // ID â†’ Token
    jieba: Jieba,                            // Jieba åˆ†è¯å™¨
    idioms: HashSet<String>,                 // æˆè¯­è¯å…¸
}

// å…¨å±€ LRU ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
lazy_static! {
    static ref TOKENIZER_CACHE: Mutex<LruCache<String, Vec<String>>> 
        = Mutex::new(LruCache::new(10000));
}
```

**å…³é”®æ–¹æ³•**:
```rust
// ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨
pub fn build_from_texts(texts: &[String]) -> Self {
    // 1. åˆ†è¯æ‰€æœ‰æ–‡æœ¬
    // 2. æå–å”¯ä¸€ token
    // 3. æ£€æµ‹æˆè¯­
    // 4. æ„å»ºåŒå‘æ˜ å°„
}

// ç¼–ç æ–‡æœ¬ä¸º token IDs
pub fn encode_sequence(&self, text: &str) -> Vec<usize> {
    // 1. æ£€æŸ¥ç¼“å­˜
    // 2. Jieba åˆ†è¯
    // 3. Token â†’ ID æ˜ å°„
    // 4. æœªçŸ¥è¯ â†’ <|unk|>
}

// è§£ç  token IDs ä¸ºæ–‡æœ¬
pub fn decode_sequence(&self, ids: &[usize]) -> String {
    // 1. ID â†’ Token æ˜ å°„
    // 2. æ‹¼æ¥æˆå¥å­
    // 3. å»é™¤ä¸­æ–‡ä¹‹é—´çš„ç©ºæ ¼
}
```

**ç¼“å­˜æ€§èƒ½**:
```rust
// è·å–ç¼“å­˜ç»Ÿè®¡
pub fn get_cache_hit_rate() -> (usize, usize, f32) {
    // è¿”å› (å‘½ä¸­æ¬¡æ•°, æœªå‘½ä¸­æ¬¡æ•°, å‘½ä¸­ç‡)
}
```

#### src/dataset_loader.rs - æ•°æ®åŠ è½½

**åŠŸèƒ½**:
- ä» JSON æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®
- ç®€å•çš„æ•°ç»„æ ¼å¼ `["å¥å­1", "å¥å­2", ...]`

**å®ç°**:
```rust
pub fn load_json_data(path: &str) -> Result<Vec<String>, String> {
    let content = fs::read_to_string(path)?;
    let data: Vec<String> = serde_json::from_str(&content)?;
    Ok(data)
}
```

---

### Performance Optimizationsï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰

#### src/fused_ops.rs - èåˆç®—å­

**åŠŸèƒ½**:
- åˆå¹¶å¤šä¸ªæ“ä½œå‡å°‘å†…å­˜åˆ†é…
- æå‡ 15-25% æ€§èƒ½

**å®ç°**:
```rust
// LayerNorm + Linear èåˆ
pub struct FusedLayerNormLinear {
    layer_norm: LayerNorm,
    linear_weight: Array2<f32>,
    linear_bias: Array1<f32>,
}

pub fn forward(&self, input: &Array2<f32>) -> Array2<f32> {
    // 1. LayerNorm
    let normed = self.layer_norm.forward(input);
    // 2. Linearï¼ˆç›´æ¥åœ¨å½’ä¸€åŒ–ç»“æœä¸Šæ“ä½œï¼Œæ— ä¸­é—´å¼ é‡ï¼‰
    normed.dot(&self.linear_weight) + &self.linear_bias
}

// GELU + Linear èåˆ
pub struct FusedGELULinear { ... }
```

#### src/position_encoding.rs - ä½ç½®ç¼–ç 

**åŠŸèƒ½**:
- æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆAttention Is All You Needï¼‰
- é¢„è®¡ç®—å¹¶ç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰

**ç®—æ³•**:
```
PE(pos, 2i)   = sin(pos / 10000^(2i / d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))
```

**å®ç°**:
```rust
pub struct PositionEncoding {
    encoding: Array2<f32>,  // é¢„è®¡ç®—çš„ (max_seq_len, embedding_dim)
}

pub fn new(max_seq_len: usize, embedding_dim: usize) -> Self {
    // æ„é€ æ—¶ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ä½ç½®ç¼–ç 
}
```

#### src/performance_monitor.rs - æ€§èƒ½ç›‘æ§

**åŠŸèƒ½**:
- å®æ—¶ç›‘æ§è®­ç»ƒæŒ‡æ ‡ï¼ˆLoss, PPL, LR, Grad Norm, Speedï¼‰
- ä¼°ç®—å‰©ä½™æ—¶é—´ï¼ˆETAï¼‰

**è¾“å‡ºç¤ºä¾‹**:
```
[é¢„è®­ç»ƒ] Epoch 10/500 | Loss: 2.345 | PPL: 10.43 | LR: 0.0009 | 
Grad: 1.234 | Speed: 15.2 samples/s | ETA: 5m 23s
```

---

### Serializationï¼ˆåºåˆ—åŒ–ï¼‰

#### src/model_serialization.rs - æ¨¡å‹ä¿å­˜/åŠ è½½

**åŠŸèƒ½**:
- äºŒè¿›åˆ¶åºåˆ—åŒ–ï¼ˆbincodeï¼‰- å¿«é€Ÿã€ç´§å‡‘
- JSON åºåˆ—åŒ–ï¼ˆserde_jsonï¼‰- å¯è¯»ã€å¯è°ƒè¯•
- ä¿å­˜å®Œæ•´çŠ¶æ€ï¼ˆå‚æ•° + ä¼˜åŒ–å™¨ + Vocabï¼‰

**ä¿å­˜å†…å®¹**:
```rust
#[derive(Serialize, Deserialize)]
pub struct ModelCheckpoint {
    vocab: Vocab,                      // è¯æ±‡è¡¨
    layer_params: Vec<LayerParams>,    // æ‰€æœ‰å±‚å‚æ•°
    optimizer_state: OptimizerState,   // Adam çŠ¶æ€
    metadata: Metadata,                // è®­ç»ƒå…ƒæ•°æ®
}
```

**å…³é”®æ–¹æ³•**:
```rust
// ä¿å­˜ä¸ºäºŒè¿›åˆ¶
pub fn save_to_file(&self, path: &str) -> Result<(), String> {
    let checkpoint = self.serialize();
    let encoded = bincode::encode_to_vec(&checkpoint, config::standard())?;
    fs::write(path, encoded)?;
}

// ä»äºŒè¿›åˆ¶åŠ è½½
pub fn load_from_file(path: &str) -> Result<LLM, String> {
    let data = fs::read(path)?;
    let (checkpoint, _) = bincode::decode_from_slice(&data, config::standard())?;
    LLM::deserialize(checkpoint)
}
```

---

## ğŸ§ª æµ‹è¯•æ–‡ä»¶è¯¦è§£

### tests/ ç›®å½•ç»“æ„

#### å•å…ƒæµ‹è¯•ï¼ˆç»„ä»¶çº§ï¼‰
- **llm_test.rs**: LLM å®Œæ•´è®­ç»ƒå’Œæ¨ç†æµç¨‹
- **transformer_test.rs**: Transformer Block å‰å‘/åå‘ä¼ æ’­
- **self_attention_test.rs**: æ³¨æ„åŠ›æœºåˆ¶è¾“å‡ºå½¢çŠ¶å’Œæ¢¯åº¦
- **feed_forward_test.rs**: å‰é¦ˆç½‘ç»œåŠŸèƒ½
- **embeddings_test.rs**: åµŒå…¥å±‚æŸ¥æ‰¾å’Œä½ç½®ç¼–ç 
- **output_projection_test.rs**: è¾“å‡ºå±‚å½¢çŠ¶
- **adam_test.rs**: ä¼˜åŒ–å™¨å‚æ•°æ›´æ–°
- **dataset_loader_test.rs**: æ•°æ®åŠ è½½æ­£ç¡®æ€§
- **position_encoding_test.rs**: ä½ç½®ç¼–ç ç”Ÿæˆ
- **vocab_test.rs**: è¯æ±‡è¡¨æ„å»ºå’Œç¼–ç 
- **chinese_tests.rs**: ä¸­æ–‡åˆ†è¯å’Œæˆè¯­æ£€æµ‹

### æµ‹è¯•å‘½åè§„èŒƒ

```rust
#[test]
fn test_<åŠŸèƒ½>_<é¢„æœŸç»“æœ>() {
    // ä¾‹å¦‚:
    // test_forward_output_shape()
    // test_backward_updates_parameters()
    // test_chinese_tokenization_correctness()
}
```

### è¿è¡Œæµ‹è¯•

```bash
# æ‰€æœ‰æµ‹è¯•
cargo test

# ç‰¹å®šæµ‹è¯•æ–‡ä»¶
cargo test --test llm_test

# ç‰¹å®šæµ‹è¯•å‡½æ•°
cargo test test_forward_output_shape

# æ˜¾ç¤ºè¾“å‡º
cargo test -- --nocapture
```

---

## ğŸ“š æ–‡æ¡£æ–‡ä»¶è¯¦è§£

### ç”¨æˆ·æ–‡æ¡£
- **README.md** / **README_zh.md**: å¿«é€Ÿå¼€å§‹ã€åŠŸèƒ½ä»‹ç»ã€å®‰è£…æŒ‡å—
- **BATCH_TRAINING.md**: æ‰¹é‡è®­ç»ƒä½¿ç”¨æ•™ç¨‹
- **PERFORMANCE_OPTIMIZATIONS.md**: æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§è¯´æ˜

### å¼€å‘æ–‡æ¡£
- **CLAUDE.md**: AI è¾…åŠ©å¼€å‘æŒ‡å—ï¼ˆæ¶æ„ã€æ•°æ®æµã€å¼€å‘æ¨¡å¼ï¼‰
- **IMPLEMENTATION_v0.4.0.md**: å½“å‰ç‰ˆæœ¬å®ç°ç¬”è®°

### è§„èŒƒæ–‡æ¡£ï¼ˆ.spec-workflow/ï¼‰
- **SPEC_WORKFLOW.md**: å®Œæ•´è§„èŒƒå’Œå·¥ä½œæµç¨‹ï¼ˆæœ¬é¡¹ç›®åˆ›å»ºï¼‰
- **TECH_STACK.md**: æŠ€æœ¯æ ˆè¯¦è§£ï¼ˆæœ¬é¡¹ç›®åˆ›å»ºï¼‰
- **PROJECT_STRUCTURE.md**: æœ¬æ–‡æ¡£

---

## ğŸ”§ é…ç½®æ–‡ä»¶è¯¦è§£

### rustfmt.toml - ä»£ç æ ¼å¼

```toml
edition = "2024"
max_width = 100        # æ¯è¡Œæœ€å¤§ 100 å­—ç¬¦
tab_spaces = 4         # ä½¿ç”¨ 4 ç©ºæ ¼ç¼©è¿›
use_small_heuristics = "Default"
```

### .gitignore - Git å¿½ç•¥è§„åˆ™

```
/target/              # Cargo ç¼–è¯‘è¾“å‡º
Cargo.lock            # ä¾èµ–é”å®šï¼ˆåº“é¡¹ç›®é€šå¸¸æäº¤ï¼‰
*.bin                 # æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶
*.json.bak            # å¤‡ä»½æ–‡ä»¶
```

---

## ğŸ“Š æ•°æ®æ–‡ä»¶è¯¦è§£

### data/pretraining_data.json

**æ ¼å¼**:
```json
[
    "åœ°çƒæ˜¯å¤ªé˜³ç³»çš„ç¬¬ä¸‰é¢—è¡Œæ˜Ÿ",
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ"
]
```

**ç‰¹ç‚¹**:
- çº¯ä¸­æ–‡çŸ¥è¯†æ€§é™ˆè¿°
- çº¦ 250 æ¡æ ·æœ¬
- ç”¨äºé¢„è®­ç»ƒé˜¶æ®µ

### data/chat_training_data.json

**æ ¼å¼**:
```json
[
    "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ã€‚",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚"
]
```

**ç‰¹ç‚¹**:
- å¯¹è¯é£æ ¼æ–‡æœ¬
- çº¦ 250 æ¡æ ·æœ¬
- ç”¨äºæŒ‡ä»¤å¾®è°ƒé˜¶æ®µ

### data/chinese_idioms.json

**æ ¼å¼**:
```json
[
    "ä¸€å¸†é£é¡º",
    "é©¬åˆ°æˆåŠŸ",
    "å¿ƒæƒ³äº‹æˆ",
    "ä¸‡äº‹å¦‚æ„"
]
```

**ç”¨é€”**:
- æˆè¯­æ£€æµ‹è¯å…¸
- Vocab æ„å»ºæ—¶ç‰¹æ®Šæ ‡è®°
- çº¦ 100+ å¸¸è§æˆè¯­

---

## ğŸš€ æ„å»ºäº§ç‰©

### target/ ç›®å½•ï¼ˆç¼–è¯‘è¾“å‡ºï¼‰

```
target/
â”œâ”€â”€ debug/                    # å¼€å‘æ„å»ºï¼ˆcargo buildï¼‰
â”‚   â””â”€â”€ llm                   # å¯æ‰§è¡Œæ–‡ä»¶ï¼ˆæœªä¼˜åŒ–ï¼‰
â”‚
â”œâ”€â”€ release/                  # å‘å¸ƒæ„å»ºï¼ˆcargo build --releaseï¼‰
â”‚   â””â”€â”€ llm                   # å¯æ‰§è¡Œæ–‡ä»¶ï¼ˆå®Œå…¨ä¼˜åŒ–ï¼‰
â”‚
â””â”€â”€ doc/                      # ç”Ÿæˆçš„æ–‡æ¡£ï¼ˆcargo docï¼‰
    â””â”€â”€ llm/
        â””â”€â”€ index.html        # HTML æ–‡æ¡£å…¥å£
```

### æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶

```
model_checkpoint.bin          # äºŒè¿›åˆ¶æ ¼å¼ï¼ˆ40-100 MBï¼‰
model_checkpoint.json         # JSON æ ¼å¼ï¼ˆå¯è¯»ä½†æ›´å¤§ï¼‰
```

---

## ğŸ“ ä»£ç åº¦é‡

### ä»£ç è§„æ¨¡ï¼ˆv0.4.0ï¼‰

| ç±»åˆ« | æ–‡ä»¶æ•° | ä»£ç è¡Œæ•°ï¼ˆçº¦ï¼‰ |
|------|-------|--------------|
| æ ¸å¿ƒç¥ç»ç½‘ç»œå±‚ | 7 | 2,500 |
| æ¨¡å‹ç¼–æ’ | 2 | 1,200 |
| è®­ç»ƒåŸºç¡€è®¾æ–½ | 4 | 1,500 |
| æ•°æ®å¤„ç† | 3 | 1,300 |
| æ€§èƒ½ä¼˜åŒ– | 3 | 800 |
| æµ‹è¯• | 11 | 2,000 |
| **æ€»è®¡** | **30** | **~9,300** |

### å¤æ‚åº¦æœ€é«˜çš„æ–‡ä»¶

1. **src/llm.rs** (~600 è¡Œ) - æ ¸å¿ƒæ¨¡å‹é€»è¾‘
2. **src/vocab.rs** (~1000 è¡Œ) - è¯æ±‡è¡¨å’Œä¸­æ–‡å¤„ç†
3. **src/self_attention.rs** (~650 è¡Œ) - æ³¨æ„åŠ›æœºåˆ¶
4. **src/main.rs** (~400 è¡Œ) - è®­ç»ƒæµç¨‹ç¼–æ’

---

## ğŸ” æ–‡ä»¶å¿«é€Ÿå¯¼èˆª

### æƒ³è¦ç†è§£...

| ç›®æ ‡ | ä¸»è¦æ–‡ä»¶ | ç›¸å…³æ–‡ä»¶ |
|------|---------|---------|
| **æ•´ä½“æ¶æ„** | lib.rs, llm.rs | CLAUDE.md |
| **è®­ç»ƒæµç¨‹** | main.rs | training_optimizations.rs |
| **å‰å‘ä¼ æ’­** | llm.rs (forward æ–¹æ³•) | å„å±‚çš„ forward æ–¹æ³• |
| **åå‘ä¼ æ’­** | llm.rs (backward æ–¹æ³•) | å„å±‚çš„ backward æ–¹æ³• |
| **æ³¨æ„åŠ›æœºåˆ¶** | self_attention.rs | transformer.rs |
| **ä¸­æ–‡å¤„ç†** | vocab.rs | chinese_tests.rs |
| **æ€§èƒ½ä¼˜åŒ–** | fused_ops.rs, position_encoding.rs | PERFORMANCE_OPTIMIZATIONS.md |
| **æ¨¡å‹ä¿å­˜** | model_serialization.rs | checkpoint_manager.rs |

### æƒ³è¦ä¿®æ”¹...

| ä¿®æ”¹ç›®æ ‡ | ä¸»è¦æ–‡ä»¶ | æ³¨æ„äº‹é¡¹ |
|---------|---------|---------|
| **æ¨¡å‹è¶…å‚æ•°** | lib.rs | éœ€ä¿è¯ EMBEDDING_DIM % NUM_HEADS == 0 |
| **è®­ç»ƒæ•°æ®** | data/*.json | çº¯ä¸­æ–‡ JSON æ•°ç»„ |
| **å­¦ä¹ ç‡ç­–ç•¥** | training_optimizations.rs | å½±å“æ”¶æ•›é€Ÿåº¦ |
| **é‡‡æ ·ç­–ç•¥** | llm.rs (generate_* æ–¹æ³•) | å½±å“ç”Ÿæˆè´¨é‡ |
| **æ·»åŠ æ–°å±‚** | src/<æ–°å±‚å>.rs + lib.rs | å®ç° Layer trait |
| **ä¼˜åŒ–å™¨** | adam.rs | ä¿®æ”¹ beta1, beta2, epsilon |

---

## ğŸ“‹ æ·»åŠ æ–°åŠŸèƒ½çš„æ­¥éª¤

### ç¤ºä¾‹ï¼šæ·»åŠ æ–°çš„ç¥ç»ç½‘ç»œå±‚

1. **åˆ›å»ºæ–‡ä»¶**: `src/new_layer.rs`
2. **å®ç°ç»“æ„ä½“**:
   ```rust
   pub struct NewLayer {
       // å‚æ•°å®šä¹‰
   }
   ```
3. **å®ç° Layer trait**:
   ```rust
   impl Layer for NewLayer {
       fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> { ... }
       fn backward(&mut self, grads: &Array2<f32>, lr: f32) -> Array2<f32> { ... }
       // ...
   }
   ```
4. **åœ¨ lib.rs ä¸­å£°æ˜**:
   ```rust
   pub mod new_layer;
   ```
5. **åœ¨ llm.rs ä¸­é›†æˆ**:
   ```rust
   self.layers.push(Box::new(NewLayer::new(...)));
   ```
6. **æ·»åŠ æµ‹è¯•**: `tests/new_layer_test.rs`
7. **æ›´æ–°æ–‡æ¡£**: CLAUDE.md, README.md

---

*æœ€åæ›´æ–°: 2024-10-25 | ç‰ˆæœ¬: v0.4.0*
